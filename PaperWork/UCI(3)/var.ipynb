{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19348543-0e2d-42ce-8f01-193961dc47bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU를 사용할 수 있습니다.\n",
      "GPU 디바이스: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# GPU를 사용할 수 있는지 확인\n",
    "gpu_available = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpu_available:\n",
    "    print(\"GPU를 사용할 수 있습니다.\")\n",
    "    # GPU 디바이스 정보 출력\n",
    "    for gpu in gpu_available:\n",
    "        print(\"GPU 디바이스:\", gpu)\n",
    "else:\n",
    "    print(\"GPU를 사용할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0af78868-3ee6-4130-8b83-338ec107f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# cnn model vary kernel size\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Flatten, Reshape, LSTM, Conv1D, MaxPooling1D, Bidirectional, ConvLSTM1D\n",
    "from tensorflow.keras.layers import Input, TimeDistributed, Conv1D, MaxPooling1D, BatchNormalization, GlobalAveragePooling1D, Activation\n",
    "from tensorflow.keras.layers import Input, TimeDistributed, Conv1D, MaxPooling1D, BatchNormalization, Activation, UpSampling1D, Add, LayerNormalization\n",
    "from tensorflow.keras.layers import add, multiply, GlobalAveragePooling1D\n",
    "from keras import  backend as K\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from statsmodels.tsa.api import VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a8a0ae2-53b2-4b15-a9a4-d9d17ae0deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def load_file(filepath):\n",
    "    dataframe = pd.read_csv(filepath, header=None, sep='\\s+')\n",
    "    return dataframe.values\n",
    "\n",
    "\n",
    "\n",
    "# load a list of files and return as a 3d numpy array\n",
    "def load_group(filenames, prefix=''):\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = np.dstack(loaded)\n",
    "    return loaded\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "    # body acceleration\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix + 'C:/Users/GACHON/Desktop/AiStudy/data/UCI HAR Dataset-20240320T080931Z-001/UCI HAR Dataset/')\n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix + 'C:/Users/GACHON/Desktop/AiStudy/data/UCI HAR Dataset-20240320T080931Z-001/UCI HAR Dataset/')\n",
    "\n",
    "    #zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "\n",
    "    print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "    '''\n",
    "    trainy_one_hot = to_categorical(trainy)\n",
    "    testy_one_hot = to_categorical(testy)\n",
    "    print(trainX.shape, trainy.shape, trainy_one_hot.shape, testX.shape, testy.shape, testy_one_hot.shape)\n",
    "    return trainX, trainy, trainy_one_hot, testX, testy, testy_one_hot\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "838f597d-2789-4eb0-b584-3239a8df4c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9) (7352, 1) (2947, 128, 9) (2947, 1)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainy, testX, testy = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b22ddae-a18d-4e2d-ae02-cb7951fdc06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape:  (7352, 128, 9)\n",
      "Y train shape:  (7352, 1)\n",
      "X test shape:  (2947, 128, 9)\n",
      "Y test shape:  (2947, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X train shape: \", trainX.shape)\n",
    "print(\"Y train shape: \", trainy.shape)\n",
    "print(\"X test shape: \", testX.shape)\n",
    "print(\"Y test shape: \", testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1707279a-ec9a-495f-bbd2-29fc33c67c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Walking: 1226\n",
      "1: WU: 1073\n",
      "2: WD: 986\n",
      "3: Sitting: 1286\n",
      "4: Standing: 1374\n",
      "5: Laying: 1407\n"
     ]
    }
   ],
   "source": [
    "print(\"0: Walking:\" ,np.where(trainy == 0)[0].size)\n",
    "print(\"1: WU:\" ,np.where(trainy == 1)[0].size)\n",
    "print(\"2: WD:\" ,np.where(trainy == 2)[0].size)\n",
    "print(\"3: Sitting:\" ,np.where(trainy == 3)[0].size)\n",
    "print(\"4: Standing:\" ,np.where(trainy == 4)[0].size)\n",
    "print(\"5: Laying:\" ,np.where(trainy == 5)[0].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e20b869e-69f1-4a1f-89c9-54d30340c8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data label statistics::\n",
      "[[   0 1226]\n",
      " [   1 1073]\n",
      " [   2  986]\n",
      " [   3 1286]\n",
      " [   4 1374]\n",
      " [   5 1407]]\n",
      "Test data label statistics::\n",
      "[[  0 496]\n",
      " [  1 471]\n",
      " [  2 420]\n",
      " [  3 491]\n",
      " [  4 532]\n",
      " [  5 537]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(trainy, return_counts=True)\n",
    "print (\"Train data label statistics::\")\n",
    "print (np.asarray((unique, counts)).T)\n",
    "\n",
    "unique, counts = np.unique(testy, return_counts=True)\n",
    "print (\"Test data label statistics::\")\n",
    "print (np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43799ccc-c813-42af-9664-fb626e9b002e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of trainX_filtered: (4067, 128, 9)\n",
      "Shape of trainy_filtered: (4067, 1)\n"
     ]
    }
   ],
   "source": [
    "# trainy에서 값이 0, 1, 2인 행의 인덱스 찾기\n",
    "indices_to_remove = np.where((trainy == 0) | (trainy == 1) | (trainy == 2))[0]\n",
    "\n",
    "# trainX와 trainy에서 해당 인덱스를 제거하여 새로운 trainX_filtered와 trainy_filtered 얻기\n",
    "trainX_filtered = np.delete(trainX, indices_to_remove, axis=0)\n",
    "trainy_filtered = np.delete(trainy, indices_to_remove, axis=0)\n",
    "\n",
    "# trainX_filtered와 trainy_filtered의 shape 확인\n",
    "print(\"Shape of trainX_filtered:\", trainX_filtered.shape)\n",
    "print(\"Shape of trainy_filtered:\", trainy_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42c3284f-39f6-4b52-8611-0c60d22db7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified trainy_filtered:\n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "# trainy에서 값이 3인 것을 0으로, 값이 4인 것을 1로, 값이 5인 것을 2로 변경\n",
    "trainy_filtered[trainy_filtered == 3] = 0\n",
    "trainy_filtered[trainy_filtered == 4] = 1\n",
    "trainy_filtered[trainy_filtered == 5] = 2\n",
    "\n",
    "# 변경된 trainy_filtered의 값 확인\n",
    "print(\"Modified trainy_filtered:\\n\", trainy_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c57dc7a-4c5a-40cb-bf26-96f9ba7099cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of testX_filtered: (1560, 128, 9)\n",
      "Shape of testy_filtered: (1560, 1)\n"
     ]
    }
   ],
   "source": [
    "# trainy에서 값이 0, 1, 2인 행의 인덱스 찾기\n",
    "indices_to_remove = np.where((testy == 0) | (testy == 1) | (testy == 2))[0]\n",
    "\n",
    "# trainX와 trainy에서 해당 인덱스를 제거하여 새로운 trainX_filtered와 trainy_filtered 얻기\n",
    "testX_filtered = np.delete(testX, indices_to_remove, axis=0)\n",
    "testy_filtered = np.delete(testy, indices_to_remove, axis=0)\n",
    "\n",
    "# trainX_filtered와 trainy_filtered의 shape 확인\n",
    "print(\"Shape of testX_filtered:\", testX_filtered.shape)\n",
    "print(\"Shape of testy_filtered:\", testy_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb11b46-92be-4d1c-803e-908872bb13a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified testy_filtered:\n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "# trainy에서 값이 3인 것을 0으로, 값이 4인 것을 1로, 값이 5인 것을 2로 변경\n",
    "testy_filtered[testy_filtered == 3] = 0\n",
    "testy_filtered[testy_filtered == 4] = 1\n",
    "testy_filtered[testy_filtered == 5] = 2\n",
    "\n",
    "# 변경된 trainy_filtered의 값 확인\n",
    "print(\"Modified testy_filtered:\\n\", testy_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0a84a89-1f73-44da-8d46-68d4dd712d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "trainy_one_hot = to_categorical(trainy_filtered)\n",
    "testy_one_hot = to_categorical(testy_filtered)\n",
    "print(trainy_filtered)\n",
    "print(testy_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "197767b2-83f0-4196-939a-5ad1f1ed1e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape:  (4067, 128, 9)\n",
      "Y train shape:  (4067, 1)\n",
      "Y train One hot shape:  (4067, 3)\n",
      "X test shape:  (1560, 128, 9)\n",
      "Y test shape:  (1560, 1)\n",
      "Y test One hot shape:  (1560, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"X train shape: \", trainX_filtered.shape)\n",
    "print(\"Y train shape: \", trainy_filtered.shape)\n",
    "print(\"Y train One hot shape: \", trainy_one_hot.shape)\n",
    "print(\"X test shape: \", testX_filtered.shape)\n",
    "print(\"Y test shape: \", testy_filtered.shape)\n",
    "print(\"Y test One hot shape: \", testy_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3d7ccc6-0c76-494b-9ec5-bcd00f6ec481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Sitting: 1286\n",
      "1: Standing: 1374\n",
      "2: Laying: 1407\n"
     ]
    }
   ],
   "source": [
    "print(\"0: Sitting:\" ,np.where(trainy_filtered == 0)[0].size)\n",
    "print(\"1: Standing:\" ,np.where(trainy_filtered == 1)[0].size)\n",
    "print(\"2: Laying:\" ,np.where(trainy_filtered == 2)[0].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "218fdd5d-e9b0-4bbf-8068-d8c185fba7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data label statistics::\n",
      "[[   0 1286]\n",
      " [   1 1374]\n",
      " [   2 1407]]\n",
      "Test data label statistics::\n",
      "[[  0 491]\n",
      " [  1 532]\n",
      " [  2 537]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(trainy_filtered, return_counts=True)\n",
    "print (\"Train data label statistics::\")\n",
    "print (np.asarray((unique, counts)).T)\n",
    "\n",
    "unique, counts = np.unique(testy_filtered, return_counts=True)\n",
    "print (\"Test data label statistics::\")\n",
    "print (np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6caf8577-7572-4f49-9314-8b0616f30241",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = trainX_filtered   # at this stage, the data includes both dynamic and static HAR data\n",
    "y_train_all = trainy_filtered\n",
    "\n",
    "X_test_all = testX_filtered\n",
    "y_test_all = testy_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aceaea30-2477-4321-b3c0-b8d585a421ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_data(trainX_filtered, testX_filtered):\n",
    " # remove overlap\n",
    " cut = int(trainX_filtered.shape[1] / 2)\n",
    " longX = trainX_filtered[:, -cut:, :]\n",
    " # flatten windows\n",
    " longX = longX.reshape((longX.shape[0] * longX.shape[1], longX.shape[2]))\n",
    " # flatten train and test\n",
    " flatTrainX = trainX_filtered.reshape((trainX_filtered.shape[0] * trainX_filtered.shape[1], trainX_filtered.shape[2]))\n",
    " flatTestX = testX_filtered.reshape((testX_filtered.shape[0] * testX_filtered.shape[1], testX_filtered.shape[2]))\n",
    "\n",
    " s = StandardScaler()\n",
    " # fit on training data\n",
    " s.fit(longX)\n",
    " # apply to training and test data\n",
    " longX = s.transform(longX)\n",
    " flatTrainX = s.transform(flatTrainX)\n",
    " flatTestX = s.transform(flatTestX)\n",
    " # reshape\n",
    " flatTrainX = flatTrainX.reshape((trainX_filtered.shape))\n",
    " flatTestX = flatTestX.reshape((testX_filtered.shape))\n",
    " return flatTrainX, flatTestX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ac01087-cd19-4bd6-8f7c-80aad8d42463",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX = scale_data(trainX_filtered, testX_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6fc5776-5918-4825-aa03-940c594fed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_val,y_train_one_hot,y_val_one_hot,y_train,y_val=train_test_split(trainX_filtered, trainy_one_hot, trainy_filtered,test_size=0.3,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "491d213e-e033-4ca7-adc6-9e904dc5a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps, n_features, n_outputs = trainX_filtered.shape[1], trainX_filtered.shape[2], trainy_one_hot.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a0a5bb3-8ba1-4a04-909a-f06b4629e271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_status = len(np.unique(y_train_one_hot))\n",
    "n_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1920108e-e0e9-4a49-ba41-12537d4ba8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX.reshape(-1, trainX.shape[-1])\n",
    "testX = testX.reshape(-1, testX.shape[-1])\n",
    "X_val = X_val.reshape(-1, X_val.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45ee12b4-bc35-4f5f-9d3d-c8631620aa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_7 (LSTM)               (None, 128, 100)          44000     \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 150)               150600    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 150)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 150)               22650     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 3)                 453       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 217,703\n",
      "Trainable params: 217,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "var_model = VAR(trainX)\n",
    "var_result = var_model.fit(maxlags=1)\n",
    "\n",
    "var_forecast_steps = 10\n",
    "var_forecast = var_result.forecast(trainX, steps=var_forecast_steps)\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(100, input_shape=(n_timesteps, n_features), return_sequences = True))\n",
    "lstm_model.add(LSTM(150))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(150, activation='relu'))\n",
    "lstm_model.add(Dense(n_outputs, activation = 'softmax'))\n",
    "lstm_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea45f39c-f8e9-4a52-be73-b08720b1650e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 520576\n  y sizes: 4067\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m      4\u001b[0m train_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mlstm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainy_one_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_one_hot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test01\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test01\\lib\\site-packages\\keras\\engine\\data_adapter.py:1851\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1845\u001b[0m         label,\n\u001b[0;32m   1846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   1847\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1848\u001b[0m         ),\n\u001b[0;32m   1849\u001b[0m     )\n\u001b[0;32m   1850\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1851\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 520576\n  y sizes: 4067\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "# validation_split = 0.1\n",
    "batch_size = 128\n",
    "\n",
    "train_epochs = 20\n",
    "history = lstm_model.fit(trainX, trainy_one_hot, epochs=train_epochs, batch_size=batch_size, verbose=True,  validation_data = (X_val, y_val_one_hot), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d1c6f-f9a3-4a64-9613-e7c6db28ede6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test01",
   "language": "python",
   "name": "test01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
