{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45439053",
   "metadata": {},
   "source": [
    "### https://github.com/seongsilheo/stress_classification_with_PPG/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa39ce",
   "metadata": {},
   "source": [
    "# Peak_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5726bc4",
   "metadata": {},
   "source": [
    "Method 1 ) local minima and maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e50f809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#주어진 데이터셋에서 평균값을 기준으로 피크를 탐지하고, 탐지된 피크들 사이 간격을 검사하여 유효한 피크만을 반환\n",
    "def threshold_peakdetection(dataset, fs):\n",
    "    \n",
    "    #print(\"dataset: \",dataset)\n",
    "    window = []\n",
    "    peaklist = []\n",
    "    ybeat = []\n",
    "    listpos = 0\n",
    "    mean = np.average(dataset)\n",
    "    TH_elapsed = np.ceil(0.36 * fs)\n",
    "    npeaks = 0\n",
    "    peakarray = []\n",
    "    \n",
    "    localaverage = np.average(dataset)\n",
    "    for datapoint in dataset:\n",
    "\n",
    "        if (datapoint < localaverage) and (len(window) < 1):\n",
    "            listpos += 1\n",
    "        elif (datapoint >= localaverage):\n",
    "            window.append(datapoint)\n",
    "            listpos += 1\n",
    "        else:\n",
    "            maximum = max(window)\n",
    "            beatposition = listpos - len(window) + (window.index(max(window)))\n",
    "            peaklist.append(beatposition)\n",
    "            window = []\n",
    "            listpos += 1\n",
    "\n",
    "            \n",
    "    ## Ignore if the previous peak was within 360 ms interval becasuse it is T-wave\n",
    "    for val in peaklist:\n",
    "        if npeaks > 0:\n",
    "            prev_peak = peaklist[npeaks - 1]\n",
    "            elapsed = val - prev_peak\n",
    "            if elapsed > TH_elapsed:\n",
    "                peakarray.append(val)\n",
    "        else:\n",
    "            peakarray.append(val)\n",
    "            \n",
    "        npeaks += 1    \n",
    "\n",
    "\n",
    "    return peaklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402e8d27",
   "metadata": {},
   "source": [
    "Method 2 ) first derivative with adaptive threshold\n",
    "\n",
    "Reference:\n",
    "\n",
    "1. Li, Bing Nan, Ming Chui Dong, and Mang I. Vai. \"On an automatic delineator for \n",
    "arterial blood pressure waveforms.\" Biomedical Signal Processing and Control 5.1 (2010): 76-81.\n",
    "\n",
    "2. Elgendi, Mohamed, et al. \"Systolic peak detection in acceleration photoplethysmograms \n",
    "measured from emergency responders in tropical conditions.\" PLoS One 8.10 (2013).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "706f5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#입력 데이터를 5초 단위로 나눠줌\n",
    "def seperate_division(data,fs):\n",
    "    divisionSet = []\n",
    "    for divisionUnit in range(0,len(data)-1,5*fs):  # index of groups (per 5sec) \n",
    "        eachDivision = data[divisionUnit: (divisionUnit+1) * 5 * fs]\n",
    "        divisionSet.append(eachDivision)\n",
    "    return divisionSet\n",
    "\n",
    "#각 블록에서 일차 도함수를 계산하여 피크를 탐지, 이 과정에서 각 블록의 첫 2초 동안의 신호 평균을 임계값으로 사용\n",
    "#신호가 임계값 이상일 때만 피크로 간주, 피크간 최소 간격이 300ms 이상이어야 함\n",
    "def first_derivative_with_adaptive_ths(data, fs):\n",
    "    \n",
    "    peak = []\n",
    "    divisionSet = seperate_division(data, fs)\n",
    "    selectiveWindow = 2 * fs\n",
    "    block_size = 5 *  fs\n",
    "    bef_idx = -300\n",
    "    \n",
    "    for divInd in range(len(divisionSet)):\n",
    "        block = divisionSet[divInd]\n",
    "        ths = np.mean(block[:selectiveWindow]) # ths: 2 seconds mean in block\n",
    "        \n",
    "        firstDeriv = block[1:] - block[:-1]\n",
    "        for i in range(1,len(firstDeriv)):\n",
    "            if  firstDeriv[i] <= 0 and firstDeriv[i-1] > 0:\n",
    "                if block[i] > ths:\n",
    "                    idx = block_size*divInd + i\n",
    "                    if idx - bef_idx > (300*fs/1000):\n",
    "                        peak.append(idx)\n",
    "                        bef_idx = idx\n",
    "                                                \n",
    "    return peak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfb1a24",
   "metadata": {},
   "source": [
    "Method 3: Slope sum function with an adaptive threshold\n",
    "\n",
    "Reference\n",
    "1. Jang, Dae-Geun, et al. \"A robust method for pulse peak determination \n",
    "in a digital volume pulse waveform with a wandering baseline.\" \n",
    "IEEE transactions on biomedical circuits and systems 8.5 (2014): 729-737.\n",
    "\n",
    "2. Jang, Dae-Geun, et al. \"A real-time pulse peak detection algorithm for \n",
    "the photoplethysmogram.\" International Journal of Electronics and Electrical Engineering 2.1 (2014): 45-49."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4133aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#현재값이 이전 값보다 크가 다음 값보다 크거나 같으면 피크\n",
    "def determine_peak_or_not(prevAmp, curAmp, nextAmp):\n",
    "    if prevAmp < curAmp and curAmp >= nextAmp:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# 피크의 시작점과 끝점을 계산    \n",
    "def onoff_set(peak, sig):     # move peak from dy signal to original signal   \n",
    "    onoffset = []\n",
    "    for p in peak:\n",
    "        for i in range(p, 0,-1):\n",
    "            if sig[i] == 0:\n",
    "                onset = i\n",
    "                break\n",
    "        for j in range(p, len(sig)):\n",
    "            if sig[j] == 0:\n",
    "                offset = j\n",
    "                break\n",
    "        if onset < offset:\n",
    "            onoffset.append([onset,offset])\n",
    "    return onoffset\n",
    "    \n",
    "\n",
    "#신호의 변화율 (미분 값)을 사용하여 피크를 탐지하고 , 이를 기반으로 정확한 피크 위치를 반환하는 전체 프로세스(위 두함수 사용)\n",
    "def slope_sum_function(data,fs):\n",
    "    dy = [0]\n",
    "    \n",
    "    dy.extend(np.diff(data))\n",
    "    #dy[dy < 0 ] = 0\n",
    "    \n",
    "    w = fs // 8\n",
    "    dy_ = [0] * w\n",
    "    for i in range(len(data)-w):\n",
    "        sum_ = np.sum(dy[i:i+w])\n",
    "        if sum_ > 0:\n",
    "            dy_.append(sum_)\n",
    "        else:\n",
    "            dy_.append(0)\n",
    "    \n",
    "    init_ths = 0.6 * np.max(dy[:3*fs])\n",
    "    ths = init_ths\n",
    "    recent_5_peakAmp = []\n",
    "    peak_ind = []\n",
    "    bef_idx = -300\n",
    "    \n",
    "    for idx in range(1,len(dy_)-1):\n",
    "        prevAmp = dy_[idx-1]\n",
    "        curAmp = dy_[idx]\n",
    "        nextAmp = dy_[idx+1]\n",
    "        if determine_peak_or_not(prevAmp, curAmp, nextAmp) == True:\n",
    "            if (idx - bef_idx) > (300 * fs /1000):  # Ignore if the previous peak was within 300 ms interval\n",
    "                if len(recent_5_peakAmp) < 100:  \n",
    "                    if curAmp > ths:\n",
    "                        peak_ind.append(idx)\n",
    "                        bef_idx = idx\n",
    "                        recent_5_peakAmp.append(curAmp)\n",
    "                elif len(recent_5_peakAmp) == 100:\n",
    "                    ths = 0.7*np.median(recent_5_peakAmp)\n",
    "                    if curAmp > ths:\n",
    "                        peak_ind.append(idx)\n",
    "                        bef_idx = idx\n",
    "                        recent_5_peakAmp.pop(0)\n",
    "                        recent_5_peakAmp.append(curAmp)\n",
    "                        \n",
    "    onoffset = onoff_set(peak_ind, dy_)\n",
    "    corrected_peak_ind = []\n",
    "    for onoff in onoffset:\n",
    "        segment = data[onoff[0]:onoff[1]]\n",
    "        corrected_peak_ind.append(np.argmax(segment) + onoff[0])\n",
    "                    \n",
    "    return corrected_peak_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e571b",
   "metadata": {},
   "source": [
    "Method 4\n",
    "\n",
    "Event-Related Moving Averages with Dynamic Threshold\n",
    "\n",
    "Method 4: 사건 관련 이동 평균과 동적 임계값\n",
    "\n",
    "Reference\n",
    "\n",
    "1. Elgendi, Mohamed, et al. \"Systolic peak detection in acceleration photoplethysmograms \n",
    "measured from emergency responders in tropical conditions.\" PLoS One 8.10 (2013).\n",
    "\n",
    "2. https://github.com/neuropsychology/NeuroKit/blob/8a2148fe477f20328d18b6da7bbb1c8438e60f18/neurokit2/signal/signal_formatpeaks.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56acc483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신호의 이동 평균을 계산\n",
    "\n",
    "# 윈도우(필터)의 종류는 boxcar(평균필터)\n",
    "def moving_average(signal, kernel='boxcar', size=5):    \n",
    "    size = int(size)\n",
    "    window = scipy.signal.get_window(kernel, size)\n",
    "    w = window / window.sum()\n",
    "    \n",
    "    # Extend signal edges to avoid boundary effects\n",
    "    x = np.concatenate((signal[0] * np.ones(size), signal, signal[-1] * np.ones(size)))\n",
    "    \n",
    "    # Compute moving average\n",
    "    smoothed = np.convolve(w, x, mode='same')\n",
    "    smoothed = smoothed[size:-size]\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "# 주어진 신호에서 피크 탐지\n",
    "def moving_averages_with_dynamic_ths(signals,sampling_rate=64, peakwindow=.111, \n",
    "                                     beatwindow=.667, beatoffset=.02, mindelay=.3,show=False):\n",
    "    if show:\n",
    "        fig, (ax0, ax1) = plt.subplots(nrow=2, ncols=1, sharex=True)\n",
    "        ax0.plot(data, label='filtered')\n",
    "    \n",
    "    signal = signals.copy()\n",
    "    # ignore the samples with n\n",
    "    signal[signal < 0] = 0\n",
    "    sqrd = signal**2\n",
    "    \n",
    "    # Compute the thresholds for peak detection. Call with show=True in order\n",
    "    # to visualize thresholds.\n",
    "    ma_peak_kernel = int(np.rint(peakwindow * sampling_rate))\n",
    "    ma_peak = moving_average(sqrd, size=ma_peak_kernel)\n",
    "    \n",
    "    ma_beat_kernel = int(np.rint(beatwindow * sampling_rate))\n",
    "    ma_beat = moving_average(sqrd, size=ma_beat_kernel)\n",
    "\n",
    "    \n",
    "    thr1 = ma_beat + beatoffset * np.mean(sqrd)    # threshold 1\n",
    "\n",
    "    if show:\n",
    "        ax1.plot(sqrd, label=\"squared\")\n",
    "        ax1.plot(thr1, label=\"threshold\")\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    # Identify start and end of PPG waves.\n",
    "    waves = ma_peak > thr1\n",
    "    \n",
    "    beg_waves = np.where(np.logical_and(np.logical_not(waves[0:-1]),\n",
    "                                        waves[1:]))[0]\n",
    "    end_waves = np.where(np.logical_and(waves[0:-1],\n",
    "                                        np.logical_not(waves[1:])))[0]\n",
    "    # Throw out wave-ends that precede first wave-start.\n",
    "    end_waves = end_waves[end_waves > beg_waves[0]]\n",
    "\n",
    "    # Identify systolic peaks within waves (ignore waves that are too short).\n",
    "    num_waves = min(beg_waves.size, end_waves.size)\n",
    "    min_len = int(np.rint(peakwindow * sampling_rate))    # threshold 2\n",
    "    min_delay = int(np.rint(mindelay * sampling_rate))\n",
    "    peaks = [0]\n",
    "\n",
    "    for i in range(num_waves):\n",
    "\n",
    "        beg = beg_waves[i]\n",
    "        end = end_waves[i]\n",
    "        len_wave = end - beg\n",
    "\n",
    "        if len_wave < min_len: # threshold 2\n",
    "            continue\n",
    "\n",
    "        # Visualize wave span.\n",
    "        if show:\n",
    "            ax1.axvspan(beg, end, facecolor=\"m\", alpha=0.5)\n",
    "\n",
    "        # Find local maxima and their prominence within wave span.\n",
    "        data = signal[beg:end]\n",
    "        locmax, props = scipy.signal.find_peaks(data, prominence=(None, None))\n",
    "\n",
    "        if locmax.size > 0:\n",
    "            # Identify most prominent local maximum.\n",
    "            peak = beg + locmax[np.argmax(props[\"prominences\"])]\n",
    "            # Enforce minimum delay between peaks(300ms)\n",
    "            if peak - peaks[-1] > min_delay:\n",
    "                peaks.append(peak)\n",
    "\n",
    "    peaks.pop(0)\n",
    "\n",
    "    if show:\n",
    "        ax0.scatter(peaks, signal[peaks], c=\"r\")\n",
    "\n",
    "    peaks = np.asarray(peaks).astype(int)\n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b87681e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def lmm_peakdetection(data,fs):\n",
    "    \n",
    "    peak_final = []\n",
    "    peaks, _ = find_peaks(data,height=0)\n",
    "    \n",
    "    for peak in peaks:\n",
    "        if data[peak] > 0:\n",
    "            peak_final.append(peak)\n",
    "        \n",
    "    return peak_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf7b4a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_peak(preprocessed_data, fs, ensemble_ths=4):\n",
    "    \n",
    "    peak1 = threshold_peakdetection(preprocessed_data,fs)\n",
    "    peak2 = slope_sum_function(preprocessed_data, fs)\n",
    "    peak3 = first_derivative_with_adaptive_ths(preprocessed_data, fs)\n",
    "    peak4 = moving_averages_with_dynamic_ths(preprocessed_data)\n",
    "    peak5 = lmm_peakdetection(preprocessed_data,fs)\n",
    "    \n",
    "    peak_dic = dict()\n",
    "\n",
    "    for key in peak1:\n",
    "        peak_dic[key] = 1\n",
    "\n",
    "    for key in peak2:\n",
    "        if key in peak_dic.keys():\n",
    "            peak_dic[key] += 1\n",
    "        else:\n",
    "            peak_dic[key] = 1\n",
    "    \n",
    "    for key in peak3:\n",
    "        if key in peak_dic.keys():\n",
    "            peak_dic[key] += 1\n",
    "        else:\n",
    "            peak_dic[key] = 1\n",
    "        \n",
    "    for key in peak4:\n",
    "        if key in peak_dic.keys():\n",
    "            peak_dic[key] += 1\n",
    "        else:\n",
    "            peak_dic[key] = 1\n",
    "        \n",
    "    for key in peak5:\n",
    "        if key in peak_dic.keys():\n",
    "            peak_dic[key] += 1\n",
    "        else:\n",
    "            peak_dic[key] = 1\n",
    "        \n",
    "    peak_dic = dict(sorted(peak_dic.items()))\n",
    "\n",
    "    count = 0\n",
    "    cnt = 0\n",
    "    bef_key = 0\n",
    "    margin = 1\n",
    "\n",
    "    new_peak_dic = dict()\n",
    "\n",
    "    for key in peak_dic.keys():\n",
    "        if cnt == 0:\n",
    "            new_peak_dic[key] = peak_dic[key]\n",
    "        else:\n",
    "            if bef_key + margin >= key:  # 마진 1안에 다음 피크가 존재하면\n",
    "                if peak_dic[bef_key] > peak_dic[key]: # 이전 피크 기준으로 개수 카운트\n",
    "                    new_peak_dic[bef_key] += peak_dic[key]\n",
    "                else:\n",
    "                    #print(\"new peak dic: \",new_peak_dic)\n",
    "                    new_peak_dic[key] = peak_dic[key] + peak_dic[bef_key] # 현재 피크 기준으로 개수 카운트\n",
    "                    del(new_peak_dic[bef_key])\n",
    "                    bef_key = key\n",
    "            else:\n",
    "                new_peak_dic[key] = peak_dic[key]\n",
    "                bef_key = key\n",
    "        cnt += 1\n",
    "    \n",
    "    ensemble_dic = dict()\n",
    "    for (key, value) in new_peak_dic.items():\n",
    "        if value >= ensemble_ths:\n",
    "            ensemble_dic[key] = value\n",
    "            \n",
    "    final_peak = list(ensemble_dic.keys())\n",
    "    \n",
    "    return final_peak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee819f7",
   "metadata": {},
   "source": [
    "# noise_reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386c9aa",
   "metadata": {},
   "source": [
    "+\n",
    "filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a69671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs  \n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpassfilter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def movingaverage(data, periods=4):\n",
    "    result = []\n",
    "    data_set = np.asarray(data)\n",
    "    weights = np.ones(periods) / periods\n",
    "    result = np.convolve(data_set, weights, mode='valid')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b307c641",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f46a0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detrend_signals(signals):\n",
    "    detrended_signals = []\n",
    "    X = [k for k in range(0, len(signals))]\n",
    "    X = np.reshape(X, (len(X), 1)) ## Reshapes the array into necessary format\n",
    "    model = LinearRegression() ## Defines the model\n",
    "    model.fit(X, signals) ## Fits signals to the model\n",
    "    trend = model.predict(X) ## Predicts the model's trend\n",
    "    detrend = [signals[k] - trend[k] for k in range(len(signals))] ## Removes trend from signal\n",
    "    detrended_signals.append(detrend)\n",
    "    \n",
    "    return detrended_signals[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8edbf4",
   "metadata": {},
   "source": [
    "Method 1 ) local minima and maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c4c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_peakdetection(dataset, fs):\n",
    "    \n",
    "    #print(\"dataset: \",dataset)\n",
    "    window = []\n",
    "    peaklist = []\n",
    "    ybeat = []\n",
    "    listpos = 0\n",
    "    mean = np.average(dataset)\n",
    "    TH_elapsed = np.ceil(0.36 * fs)\n",
    "    npeaks = 0\n",
    "    peakarray = []\n",
    "    \n",
    "    localaverage = np.average(dataset)\n",
    "    for datapoint in dataset:\n",
    "\n",
    "        if (datapoint < localaverage) and (len(window) < 1):\n",
    "            listpos += 1\n",
    "        elif (datapoint >= localaverage):\n",
    "            window.append(datapoint)\n",
    "            listpos += 1\n",
    "        else:\n",
    "            maximum = max(window)\n",
    "            beatposition = listpos - len(window) + (window.index(max(window)))\n",
    "            peaklist.append(beatposition)\n",
    "            window = []\n",
    "            listpos += 1\n",
    "\n",
    "            \n",
    "    ## Ignore if the previous peak was within 360 ms interval becasuse it is T-wave\n",
    "    for val in peaklist:\n",
    "        if npeaks > 0:\n",
    "            prev_peak = peaklist[npeaks - 1]\n",
    "            elapsed = val - prev_peak\n",
    "            if elapsed > TH_elapsed:\n",
    "                peakarray.append(val)\n",
    "        else:\n",
    "            peakarray.append(val)\n",
    "            \n",
    "        npeaks += 1    \n",
    "\n",
    "\n",
    "    return peaklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b78e31e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RR_interval(peaklist,fs):\n",
    "    RR_list = []\n",
    "    cnt = 0\n",
    "    while (cnt < (len(peaklist)-1)):\n",
    "        RR_interval = (peaklist[cnt+1] - peaklist[cnt]) # Calculate distance between beats in # of samples\n",
    "        ms_dist = ((RR_interval / fs) * 1000.0)  # Convert sample distances to ms distances (fs로 나눠서 1초단위로 거리표현 -> 1ms단위로 change) \n",
    "        RR_list.append(ms_dist)\n",
    "        cnt += 1\n",
    "        \n",
    "    return RR_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "926c70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_heartrate(RR_list):\n",
    "    HR = []\n",
    "    heartrate_array=[]\n",
    "    window_size = 10\n",
    "\n",
    "    for val in RR_list:\n",
    "        if val > 400 and val < 1500:\n",
    "            heart_rate = 60000.0 / val #60000 ms /1 minute. time per beat(한번 beat하는데 걸리는 시간)\n",
    "            \n",
    "        # if RR-interval < .1905 seconds, heart-rate > highest recorded value, 315 BPM. Probably an error!\n",
    "        elif (val > 0 and val < 400) or val > 1500:\n",
    "            if len(HR) > 0:\n",
    "                # ... and use the mean heart-rate from the data so far:\n",
    "                heart_rate = np.mean(HR[-window_size:])\n",
    "\n",
    "            else:\n",
    "                heart_rate = 60.0\n",
    "        else:\n",
    "            # Get around divide by 0 error\n",
    "            heart_rate = 0.0\n",
    "\n",
    "        HR.append(heart_rate)\n",
    "\n",
    "    return HR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8c65de",
   "metadata": {},
   "source": [
    "1. Frequency view\n",
    "\n",
    "Reference:\n",
    "Sadhukhan, Deboleena, Saurabh Pal, and Madhuchhanda Mitra. \n",
    "\"PPG Noise Reduction based on Adaptive Frequency Suppression using Discrete Fourier Transform \n",
    "for Portable Home Monitoring Applications.\" \n",
    "2018 15th IEEE India Council International Conference (INDICON). IEEE, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "729238f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT(block,fs):\n",
    "    fourierTransform = np.fft.fft(block)/len(block)  # divide by len(block) to normalize\n",
    "    fourierTransform = fourierTransform[range(int(len(block)/2))] # single side frequency / symmetric\n",
    "\n",
    "    tpCount = len(block)\n",
    "    values = np.arange(int(tpCount)/2)\n",
    "\n",
    "    timePeriod = tpCount / fs\n",
    "    frequencies = values/timePeriod # frequency components\n",
    "\n",
    "    '''\n",
    "    plt.figure(figsize=(40,20))\n",
    "    plt.plot(frequencies, abs(fourierTransform)) \n",
    "\n",
    "    plt.xticks(fontsize=30)\n",
    "    plt.yticks(fontsize=30)\n",
    "    plt.xlabel(\"frequency(Hz)\",fontsize=50)\n",
    "    plt.ylabel(\"FFT magnitude(mV)\",fontsize=50)\n",
    "    '''\n",
    "    \n",
    "    return frequencies, fourierTransform, timePeriod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "447932b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cutoff(block,fs):\n",
    "    \n",
    "    block = np.array([item.real for item in block])\n",
    "    peak = threshold_peakdetection(block,fs)\n",
    "    hr_mean = np.mean(calc_heartrate(RR_interval(peak,fs)))\n",
    "    #low_cutoff = np.round(hr_mean / 60 - 0.2, 1)\n",
    "    low_cutoff = np.round(hr_mean / 60 - 0.6, 1) # 0.6\n",
    "    \n",
    "    \n",
    "    frequencies, fourierTransform, timePeriod = FFT(block,fs)\n",
    "    ths = max(abs(fourierTransform)) * 0.10\n",
    "    \n",
    "    for i in range(int(5*timePeriod),0, -1):  # check from 5th harmonic\n",
    "        if abs(fourierTransform[i]) > ths:\n",
    "            high_cutoff = np.round(i/timePeriod, 1) \n",
    "            break\n",
    "    \n",
    "    print('low cutoff: ', low_cutoff, 'high_cutoff: ', high_cutoff)\n",
    "    \n",
    "    #low_loc = np.where(frequencies == low_cutoff)[0][0]\n",
    "    #high_loc = np.where(frequencies == high_cutoff)[0][0]\n",
    "            \n",
    "    return [low_cutoff, high_cutoff]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6eeee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_reconstruction_dft(data, fs, sec, overlap, cutoff):\n",
    "    concatenated_sig = []\n",
    "    \n",
    "    for i in range(0, len(data), fs*sec-overlap):\n",
    "        seg_data = data[i:i+fs*sec]\n",
    "        sig_fft = fftpack.fft(seg_data)\n",
    "    \n",
    "        \n",
    "        #The corresponding frequencies\n",
    "        sample_freq = (fftpack.fftfreq(len(seg_data)) * fs)\n",
    "\n",
    "        new_freq_fft = sig_fft.copy()\n",
    "        new_freq_fft[np.abs(sample_freq) < cutoff[0]] = 0\n",
    "        new_freq_fft[np.abs(sample_freq) > cutoff[1]] = 0\n",
    "    \n",
    "        filtered_sig = fftpack.ifft(new_freq_fft)\n",
    "        \n",
    "        # 2% overlapping\n",
    "        if i == 0:\n",
    "            concatenated_sig = np.hstack([concatenated_sig, filtered_sig[:fs*sec - overlap//2]])\n",
    "        elif i == len(data)-1:\n",
    "            concatenated_sig = np.hstack(concatenated_sig, filtered_sig[overlap//2:])\n",
    "        else:\n",
    "            concatenated_sig = np.hstack([concatenated_sig, filtered_sig[overlap//2:fs*sec - overlap//2]])\n",
    "        \n",
    "    return concatenated_sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f81f4b",
   "metadata": {},
   "source": [
    "+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb75a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def ppg_normalization(data, min_range, max_range):\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(min_range,max_range))\n",
    "    scaler.fit(data)\n",
    "    norm_data = scaler.transform(data)\n",
    "    \n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d61d5a7",
   "metadata": {},
   "source": [
    "Reference:\n",
    "Hanyu, Shao, and Chen Xiaohui. \"Motion artifact detection and reduction in PPG signals based on statistics analysis.\" \n",
    "2017 29th Chinese control and decision conference (CCDC). IEEE, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8e7a5",
   "metadata": {},
   "source": [
    "cycle: per period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fa08e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic_threshold(clean_signal, fs, ths):\n",
    "    stds, kurtosiss, skews, valley = statistic_detection(clean_signal, fs)\n",
    "    std_ths = np.mean(stds) + ths[0] # paper's threshold : 3.2  \n",
    "    kurt_ths = np.mean(kurtosiss) + ths[1]  #3.1  \n",
    "    skews_ths = [np.mean(skews) - ths[2], np.mean(skews) + ths[3]]  # -0.3 and 0.8   -0.5 , 0.9\n",
    "    \n",
    "    return std_ths, kurt_ths, skews_ths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe886bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic_detection(signal, fs):\n",
    "    \n",
    "    valley = pair_valley(valley_detection(signal, fs))\n",
    "    stds=[]\n",
    "    kurtosiss=[]\n",
    "    skews=[]\n",
    "\n",
    "    for val in valley: # 사이클 한 번동안의 통계적 평균 리스트 저장\n",
    "        stds.append(np.std(signal[val[0]:val[1]]))\n",
    "        kurtosiss.append(kurtosis(signal[val[0]:val[1]]))\n",
    "        skews.append(skew(signal[val[0]:val[1]])) \n",
    "\n",
    "    return stds, kurtosiss, skews, valley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ac164b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_noise_in_time(data, fs, ths,cycle=1):\n",
    "    stds, kurtosiss,skews, valley = statistic_detection(data, fs)\n",
    "    \n",
    "    \n",
    "    #cycle 수만큼 다시 평균내서 리스트 저장\n",
    "    stds_, kurtosiss_, skews_ = [], [], []\n",
    "    stds_ = [np.mean(stds[i:i+cycle]) for i in range(0,len(stds)-cycle+1,cycle)]\n",
    "    kurtosiss_ = [np.mean(kurtosiss[i:i+cycle]) for i in range(0,len(kurtosiss)-cycle+1,cycle)]\n",
    "    skews_ = [np.mean(skews[i:i+cycle]) for i in range(0,len(skews)-cycle+1,cycle)]    \n",
    "   \n",
    "    # extract clean index, 사이클 인덱스       \n",
    "    eli_std = [stds_.index(x) for x in stds_ if x < ths[0]]\n",
    "    eli_kurt = [kurtosiss_.index(x) for x in kurtosiss_ if x < ths[1]]\n",
    "    eli_skew = [skews_.index(x) for x in skews_ if x > ths[2][0] and x < ths[2][1]]\n",
    "\n",
    "    total_list = eli_std + eli_kurt + eli_skew\n",
    "    \n",
    "     # store the number of extracted each index(각 인덱스 extract된 횟수 저장)\n",
    "    dic = dict()\n",
    "    for i in total_list:\n",
    "        if i in dic.keys():\n",
    "            dic[i] += 1\n",
    "        else:\n",
    "            dic[i] = 1\n",
    "            \n",
    "    new_list = []\n",
    "    for key, value in dic.items():\n",
    "        if value >= 3:\n",
    "            new_list.append(key)\n",
    "    new_list.sort()\n",
    "    \n",
    "    eliminated_data = []\n",
    "    index = []\n",
    "    for x in new_list:\n",
    "        index.extend([x for x in range(valley[x*cycle][0],valley[x*cycle+cycle-1][1],1)])\n",
    "\n",
    "    print(len(data), len(index))\n",
    "    return len(data), len(index), index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259465d4",
   "metadata": {},
   "source": [
    "+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7c884c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valley_detection(dataset, fs):\n",
    "    window = []\n",
    "    valleylist = []\n",
    "    ybeat = []\n",
    "    listpos = 0\n",
    "    TH_elapsed = np.ceil(0.36 * fs)\n",
    "    nvalleys = 0\n",
    "    valleyarray = []\n",
    "    \n",
    "    localaverage = np.average(dataset)\n",
    "    for datapoint in dataset:\n",
    "\n",
    "        if (datapoint > localaverage) and (len(window) < 1):\n",
    "            listpos += 1\n",
    "        elif (datapoint <= localaverage):\n",
    "            window.append(datapoint)\n",
    "            listpos += 1\n",
    "        else:\n",
    "            minimum = min(window)\n",
    "            beatposition = listpos - len(window) + (window.index(min(window)))\n",
    "            valleylist.append(beatposition)\n",
    "            window = []\n",
    "            listpos += 1\n",
    "\n",
    "    ## Ignore if the previous peak was within 360 ms interval becasuse it is T-wave\n",
    "    for val in valleylist:\n",
    "        if nvalleys > 0:\n",
    "            prev_valley = valleylist[nvalleys - 1]\n",
    "            elapsed = val - prev_valley\n",
    "            if elapsed > TH_elapsed:\n",
    "                valleyarray.append(val)\n",
    "        else:\n",
    "            valleyarray.append(val)\n",
    "            \n",
    "        nvalleys += 1    \n",
    "\n",
    "    return valleyarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b866bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_valley(valley):\n",
    "    pair_valley=[]\n",
    "    for i in range(len(valley)-1):\n",
    "        pair_valley.append([valley[i], valley[i+1]])\n",
    "    return pair_valley"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84196e0",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efa9d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_correlation(filtered_3sec):\n",
    "    \n",
    "    a = plt.acorr(filtered_3sec, usevlines=True, normed=True, maxlags=(len(filtered_3sec)-1), lw=2)\n",
    "    valley_indexes = signal.argrelextrema(a[1], np.less)  # find valley index\n",
    "    plt.scatter(x=valley_indexes[0]-(len(filtered_3sec)-1), y=a[1][valley_indexes[0]], color='red', s=20)\n",
    "    \n",
    "    diff = []\n",
    "    for i in range(len(valley_indexes[0]) -1):\n",
    "        diff.append(valley_indexes[0][i+1] - valley_indexes[0][i])\n",
    "    \n",
    "    return np.average(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862142bd",
   "metadata": {},
   "source": [
    "+\n",
    "it doesn't work well when they are noisy signals, a and b gotta be same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0f9595d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef improved_moving_window_method(data, fs, mean_period):\\n    \\n    # find first valley\\n    valley = valley_detection(data[:100],fs)\\n    final_valley = [valley[0]]\\n    \\n    i=0\\n    while True:\\n        if i == 0:\\n            meuw = mean_period\\n            a = math.ceil(valley[0] + 2/3*meuw)\\n            b = math.ceil(valley[0] + 4/3*meuw)\\n        else:\\n            meuw = final_valley[-1] - final_valley[-2]\\n            a = math.ceil(final_valley[-1] + 2/3*meuw)\\n            b = math.ceil(final_valley[-1] + 4/3*meuw)\\n\\n        final_valley.append(a + np.argmin(data[a:b]))      \\n        i += 1   \\n                        \\n        if b > len(data):\\n            break\\n    \\n    return final_valley\\n# + {}\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def improved_moving_window_method(data, fs, mean_period):\n",
    "    \n",
    "    # find first valley\n",
    "    valley = valley_detection(data[:100],fs)\n",
    "    final_valley = [valley[0]]\n",
    "    \n",
    "    i=0\n",
    "    while True:\n",
    "        if i == 0:\n",
    "            meuw = mean_period\n",
    "            a = math.ceil(valley[0] + 2/3*meuw)\n",
    "            b = math.ceil(valley[0] + 4/3*meuw)\n",
    "        else:\n",
    "            meuw = final_valley[-1] - final_valley[-2]\n",
    "            a = math.ceil(final_valley[-1] + 2/3*meuw)\n",
    "            b = math.ceil(final_valley[-1] + 4/3*meuw)\n",
    "\n",
    "        final_valley.append(a + np.argmin(data[a:b]))      \n",
    "        i += 1   \n",
    "                        \n",
    "        if b > len(data):\n",
    "            break\n",
    "    \n",
    "    return final_valley\n",
    "# + {}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f67dca1",
   "metadata": {},
   "source": [
    "# feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe843ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"stress_classification_with_PPG/preprocessing_tool/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3aad4494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_RRI(peaklist, fs):\n",
    "    RR_list = []\n",
    "    RR_list_e = []\n",
    "    cnt = 0\n",
    "    while (cnt < (len(peaklist)-1)):\n",
    "        RR_interval = (peaklist[cnt+1] - peaklist[cnt]) #Calculate distance between beats in # of samples\n",
    "        ms_dist = ((RR_interval / fs) * 1000.0)  #fs로 나눠서 1초단위로 거리표현 -> 1ms단위로 change /  Convert sample distances to ms distances\n",
    "        cnt += 1\n",
    "        RR_list.append(ms_dist)\n",
    "    mean_RR = np.mean(RR_list)\n",
    "\n",
    "    for ind, rr in enumerate(RR_list):\n",
    "        if rr >  mean_RR - 300 and rr < mean_RR + 300:\n",
    "            RR_list_e.append(rr)\n",
    "            \n",
    "    RR_diff = []\n",
    "    RR_sqdiff = []\n",
    "    cnt = 0\n",
    "    while (cnt < (len(RR_list_e)-1)):\n",
    "        RR_diff.append(abs(RR_list_e[cnt] - RR_list_e[cnt+1]))\n",
    "        RR_sqdiff.append(math.pow(RR_list_e[cnt] - RR_list_e[cnt+1], 2))\n",
    "        cnt += 1\n",
    "        \n",
    "    return RR_list_e, RR_diff, RR_sqdiff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22071dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_heartrate(RR_list):\n",
    "    HR = []\n",
    "    heartrate_array=[]\n",
    "    window_size = 10\n",
    "\n",
    "    for val in RR_list:\n",
    "        if val > 400 and val < 1500:\n",
    "            heart_rate = 60000.0 / val #60000 ms (1 minute) / 한번 beat하는데 걸리는 시간\n",
    "        # if RR-interval < .1905 seconds, heart-rate > highest recorded value, 315 BPM. Probably an error!\n",
    "        elif (val > 0 and val < 400) or val > 1500:\n",
    "            if len(HR) > 0:\n",
    "                # ... and use the mean heart-rate from the data so far:\n",
    "                heart_rate = np.mean(HR[-window_size:])\n",
    "\n",
    "            else:\n",
    "                heart_rate = 60.0\n",
    "        else:\n",
    "            # Get around divide by 0 error\n",
    "            print(\"err\")\n",
    "            heart_rate = 0.0\n",
    "\n",
    "        HR.append(heart_rate)\n",
    "\n",
    "    return HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12b451f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_stats_original(ppg_seg, window_length, label=-1):  # Nan을 제외하고 평균 냄 \n",
    "    \n",
    "    fs = 64   \n",
    "    \n",
    "    peak = threshold_peakdetection(ppg_seg, fs)\n",
    "    RR_list, RR_diff, RR_sqdiff = calc_RRI(peak, fs)\n",
    "    \n",
    "    # Time\n",
    "    HR = calc_heartrate(RR_list)\n",
    "    HR_mean, HR_std = np.mean(HR), np.std(HR)\n",
    "    SD_mean, SD_std = np.mean(RR_diff) , np.std(RR_diff)\n",
    "    NN50 = [x for x in RR_diff if x > 50]\n",
    "    pNN50 = len(NN50) / window_length\n",
    "    bar_y, bar_x = np.histogram(RR_list)\n",
    "    TINN = np.max(bar_x) - np.min(bar_x)\n",
    "    RMSSD = np.sqrt(np.mean(RR_sqdiff))\n",
    "    \n",
    "    # Frequency\n",
    "    rr_x = []\n",
    "    pointer = 0\n",
    "    for x in RR_list:\n",
    "        pointer += x\n",
    "        rr_x.append(pointer)\n",
    "    RR_x_new = np.linspace(rr_x[0], rr_x[-1], int(rr_x[-1]))\n",
    "    \n",
    "    if len(rr_x) <= 5 or len(RR_list) <= 5:\n",
    "        print(\"rr_x or RR_list less than 5\")   \n",
    "    \n",
    "   \n",
    "    interpolated_func = UnivariateSpline(rr_x, RR_list, k=3)\n",
    "    \n",
    "    datalen = len(RR_x_new)\n",
    "    frq = np.fft.fftfreq(datalen, d=((1/1000.0)))\n",
    "    frq = frq[range(int(datalen/2))]\n",
    "    Y = np.fft.fft(interpolated_func(RR_x_new))/datalen\n",
    "    Y = Y[range(int(datalen/2))]\n",
    "    psd = np.power(Y, 2)  # power spectral density\n",
    "\n",
    "    lf = np.trapz(abs(psd[(frq >= 0.04) & (frq <= 0.15)])) #Slice frequency spectrum where x is between 0.04 and 0.15Hz (LF), and use NumPy's trapezoidal integration function to find the are\n",
    "    hf = np.trapz(abs(psd[(frq > 0.15) & (frq <= 0.5)])) #Do the same for 0.16-0.5Hz (HF)\n",
    "    ulf = np.trapz(abs(psd[frq < 0.003]))\n",
    "    vlf = np.trapz(abs(psd[(frq >= 0.003) & (frq < 0.04)]))\n",
    "    \n",
    "    if hf != 0:\n",
    "        lfhf = lf/hf\n",
    "    else:\n",
    "        lfhf = 0\n",
    "        \n",
    "    total_power = lf + hf + vlf\n",
    "\n",
    "    features = {'HR_mean': HR_mean, 'HR_std': HR_std, 'SD_mean': SD_mean, 'SD_std': SD_std, 'pNN50': pNN50, 'TINN': TINN, 'RMSSD': RMSSD,\n",
    "                'LF': lf, 'HF': hf, 'ULF' : ulf, 'VLF': vlf, 'LFHF': lfhf, 'Total_power': total_power, 'label': label}\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5077318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_entropy(U, m=2, r=3):\n",
    "\n",
    "    def _maxdist(x_i, x_j):\n",
    "        return max([abs(ua - va) for ua, va in zip(x_i, x_j)])\n",
    "\n",
    "    def _phi(m):\n",
    "        x = [[U[j] for j in range(i, i + m - 1 + 1)] for i in range(N - m + 1)]\n",
    "        C = [len([1 for x_j in x if _maxdist(x_i, x_j) <= r]) / (N - m + 1.0) for x_i in x]\n",
    "        return (N - m + 1.0)**(-1) * sum(np.log(C))\n",
    "\n",
    "    N = len(U)\n",
    "\n",
    "    return abs(_phi(m+1) - _phi(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "938b2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(signal):\n",
    "    #signal = list(signal)\n",
    "    \n",
    "    data_set = list(set(signal))\n",
    "    freq_list = []\n",
    "    for entry in data_set:\n",
    "        counter = 0.\n",
    "        for i in signal:\n",
    "            if i == entry:\n",
    "                counter += 1\n",
    "        freq_list.append(float(counter) / len(signal))\n",
    "        \n",
    "    ent = 0.0\n",
    "    for freq in freq_list:\n",
    "        ent += freq * np.log2(freq)\n",
    "    \n",
    "    ent = -ent\n",
    "    \n",
    "    return ent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0496faf",
   "metadata": {},
   "source": [
    " https://horizon.kias.re.kr/12415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a1e7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_entropy(sig,ordr,tor):\n",
    "    # sig: the input signal or series, it should be numpy array with type float\n",
    "    # ordr: order, the length of template,embedding dimension\n",
    "    # tor: percent of standard deviation\n",
    "    \n",
    "    sig = np.array(sig)\n",
    "    n = len(sig)\n",
    "    #tor = np.std(sig)*tor\n",
    "    \n",
    "    matchnum = 0.0\n",
    "    for i in range(n-ordr):\n",
    "        tmpl = sig[i:i+ordr] # generate samples length ordr\n",
    "        for j in range (i+1,n-ordr+1): \n",
    "            ltmp = sig[j:j+ordr]\n",
    "            diff = tmpl-ltmp  # measure mean similarity\n",
    "            if all(diff<tor):\n",
    "                matchnum+=1\n",
    "    \n",
    "    allnum = (n-ordr+1)*(n-ordr)/2\n",
    "    if matchnum<0.1:\n",
    "        sen = 1000.0\n",
    "    else:\n",
    "        sen = -math.log(matchnum/allnum)\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "629194e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_td_hrv(RR_list, RR_diff, RR_sqdiff, window_length): \n",
    "    \n",
    "    # Time\n",
    "    HR = calc_heartrate(RR_list)\n",
    "    HR_mean, HR_std = np.mean(HR), np.std(HR)\n",
    "    meanNN, SDNN, medianNN = np.mean(RR_list), np.std(RR_list), np.median(np.abs(RR_list))\n",
    "    meanSD, SDSD = np.mean(RR_diff) , np.std(RR_diff)\n",
    "    RMSSD = np.sqrt(np.mean(RR_sqdiff))\n",
    "    \n",
    "    NN20 = [x for x in RR_diff if x > 20]\n",
    "    NN50 = [x for x in RR_diff if x > 50]\n",
    "    pNN20 = len(NN20) / window_length\n",
    "    pNN50 = len(NN50) / window_length\n",
    "    \n",
    "    \n",
    "    bar_y, bar_x = np.histogram(RR_list)\n",
    "    TINN = np.max(bar_x) - np.min(bar_x)\n",
    "    \n",
    "    RMSSD = np.sqrt(np.mean(RR_sqdiff))\n",
    "    \n",
    "\n",
    "    features = {'HR_mean': HR_mean, 'HR_std': HR_std, 'meanNN': meanNN, 'SDNN': SDNN, 'medianNN': medianNN,\n",
    "                'meanSD': meanSD, 'SDSD': SDSD, 'RMSSD': RMSSD, 'pNN20': pNN20, 'pNN50': pNN50, 'TINN': TINN}\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "626fcb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fd_hrv(RR_list):  \n",
    "    \n",
    "    rr_x = []\n",
    "    pointer = 0\n",
    "    for x in RR_list:\n",
    "        pointer += x\n",
    "        rr_x.append(pointer)\n",
    "        \n",
    "    if len(rr_x) <= 3 or len(RR_list) <= 3:\n",
    "        print(\"rr_x or RR_list less than 5\")   \n",
    "        return 0\n",
    "    \n",
    "    RR_x_new = np.linspace(rr_x[0], rr_x[-1], int(rr_x[-1]))\n",
    "    \n",
    "   \n",
    "    interpolated_func = UnivariateSpline(rr_x, RR_list, k=3)\n",
    "    \n",
    "    datalen = len(RR_x_new)\n",
    "    frq = np.fft.fftfreq(datalen, d=((1/1000.0)))\n",
    "    frq = frq[range(int(datalen/2))]\n",
    "    Y = np.fft.fft(interpolated_func(RR_x_new))/datalen\n",
    "    Y = Y[range(int(datalen/2))]\n",
    "    psd = np.power(Y, 2)  # power spectral density\n",
    "\n",
    "    lf = np.trapz(abs(psd[(frq >= 0.04) & (frq <= 0.15)])) #Slice frequency spectrum where x is between 0.04 and 0.15Hz (LF), and use NumPy's trapezoidal integration function to find the are\n",
    "    hf = np.trapz(abs(psd[(frq > 0.15) & (frq <= 0.5)])) #Do the same for 0.16-0.5Hz (HF)\n",
    "    ulf = np.trapz(abs(psd[frq < 0.003]))\n",
    "    vlf = np.trapz(abs(psd[(frq >= 0.003) & (frq < 0.04)]))\n",
    "    \n",
    "    if hf != 0:\n",
    "        lfhf = lf/hf\n",
    "    else:\n",
    "        lfhf = 0\n",
    "        \n",
    "    total_power = lf + hf + vlf\n",
    "    lfp = lf / total_power\n",
    "    hfp = hf / total_power\n",
    "\n",
    "    features = {'LF': lf, 'HF': hf, 'ULF' : ulf, 'VLF': vlf, 'LFHF': lfhf, 'total_power': total_power, 'lfp': lfp, 'hfp': hfp}\n",
    "    bef_features = features\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e25533f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_nonli_hrv(RR_list,label): \n",
    "    \n",
    "    diff_RR = np.diff(RR_list)\n",
    "    sd_heart_period = np.std(diff_RR, ddof=1) ** 2\n",
    "    SD1 = np.sqrt(sd_heart_period * 0.5)\n",
    "    SD2 = 2 * sd_heart_period - 0.5 * sd_heart_period\n",
    "    pA = SD1*SD2\n",
    "    \n",
    "    if SD2 != 0:\n",
    "        pQ = SD1 / SD2\n",
    "    else:\n",
    "        print(\"SD2 is zero\")\n",
    "        pQ = 0\n",
    "    \n",
    "    ApEn = approximate_entropy(RR_list,2,3)  \n",
    "    shanEn = shannon_entropy(RR_list)\n",
    "    #sampEn = nolds.sampen(RR_list,emb_dim=2)\n",
    "    D2 = nolds.corr_dim(RR_list, emb_dim=2)\n",
    "    #dfa1 = nolds.dfa(RR_list, range(4,17))\n",
    "    # dfa2 = nolds.dfa(RR_list, range(16,min(len(RR_list)-1, 66)))\n",
    "    #dimension, delay, threshold, norm, minimum_diagonal_line_length = 3, 2, 0.7, \"manhattan\", 2\n",
    "    #rec_mat = recurrence_matrix(RR_list, dimension, delay, threshold, norm)\n",
    "    #REC, RPImean, RPImax, RPadet = recurrence_quantification_analysis(rec_mat, minimum_diagonal_line_length)\n",
    "    # recurrence_rate, average_diagonal_line_length, longest_diagonal_line_length, determinism\n",
    "\n",
    "    features = {'SD1': SD1, 'SD2': SD2, 'pA': pA, 'pQ': pQ, 'ApEn' : ApEn, 'shanEn': shanEn, 'D2': D2, \n",
    "                'label': label}\n",
    "    # 'dfa1': dfa1, 'dfa2': dfa2, 'REC': REC, 'RPImean': RPImean, 'RPImax': RPImax, 'RPadet': RPadet,\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f53eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_stats_27_features(ppg_seg, window_length, label, ensemble, ma_usage):  \n",
    "    \n",
    "    fs = 64  \n",
    "    \n",
    "    if ma_usage:\n",
    "        fwd = moving_average(ppg_seg, size=3)\n",
    "        bwd = moving_average(ppg_seg[::-1], size=3)\n",
    "        ppg_seg = np.mean(np.vstack((fwd,bwd[::-1])), axis=0)\n",
    "    ppg_seg = np.array([item.real for item in ppg_seg])\n",
    "    \n",
    "    #peak = threshold_peakdetection(ppg_seg, fs)\n",
    "    #peak = first_derivative_with_adaptive_ths(ppg_seg, fs)\n",
    "    #peak = slope_sum_function(ppg_seg, fs)\n",
    "    #peak = moving_averages_with_dynamic_ths(ppg_seg)\n",
    "    peak = lmm_peakdetection(ppg_seg,fs)\n",
    "\n",
    "        \n",
    "    if ensemble:\n",
    "        ensemble_ths = 3\n",
    "        #print(\"one algorithm peak length: \", len(peak))\n",
    "        peak = ensemble_peak(ppg_seg, fs, ensemble_ths)\n",
    "        #print(\"after ensemble peak length: \", len(peak))\n",
    "        \n",
    "        if(len(peak) < 100):\n",
    "            print(\"skip\")\n",
    "            return []\n",
    "\n",
    "        \n",
    "    RR_list, RR_diff, RR_sqdiff = calc_RRI(peak, fs)\n",
    "    #print(RR_list)\n",
    "    \n",
    "    if len(RR_list) <= 3:\n",
    "        return []\n",
    "    \n",
    "    td_features = calc_td_hrv(RR_list, RR_diff, RR_sqdiff, window_length)\n",
    "    fd_features = calc_fd_hrv(RR_list)\n",
    "    \n",
    "    if fd_features == 0:\n",
    "        return []\n",
    "    nonli_features = calc_nonli_hrv(RR_list,label)\n",
    "    \n",
    "    total_features = {**td_features, **fd_features, **nonli_features}\n",
    "    \n",
    "    \n",
    "    return total_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189ea30",
   "metadata": {},
   "source": [
    "# read_data_new_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1bb4cd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nolds\n",
      "  Downloading nolds-0.6.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from nolds) (1.24.4)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from nolds) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nolds) (69.0.3)\n",
      "Downloading nolds-0.6.0-py2.py3-none-any.whl (223 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nolds\n",
      "Successfully installed nolds-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ce6f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import nolds\n",
    "\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy import stats\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "833b99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy import signal\n",
    "from scipy import fft, ifft\n",
    "from scipy import fftpack\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0a67d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # +\n",
    "WINDOW_IN_SECONDS = 120  # 120 / 180 / 300\n",
    "\n",
    "NOISE = ['bp_time_ens']\n",
    "main_path='WESAD/WESAD/'\n",
    "\n",
    "# +\n",
    "# E4 (wrist) Sampling Frequencies\n",
    "\n",
    "fs_dict = {'ACC': 32, 'BVP': 64, 'EDA': 4, 'TEMP': 4, 'label': 700, 'Resp': 700}\n",
    "\n",
    "label_dict = {'baseline': 1, 'stress': 2, 'amusement': 0}\n",
    "int_to_label = {1: 'baseline', 2: 'stress', 0: 'amusement'}\n",
    "    \n",
    "sec = 12\n",
    "N = fs_dict['BVP']*sec  # one block : 10 sec\n",
    "overlap = int(np.round(N * 0.02)) # overlapping length\n",
    "overlap = overlap if overlap%2 ==0 else overlap+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd91eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectData:\n",
    "\n",
    "    def __init__(self, main_path, subject_number):\n",
    "        self.name = f'S{subject_number}'\n",
    "        self.subject_keys = ['signal', 'label', 'subject']\n",
    "        self.signal_keys = ['chest', 'wrist']\n",
    "        self.chest_keys = ['ACC', 'ECG', 'EMG', 'EDA', 'Temp', 'Resp']\n",
    "        self.wrist_keys = ['ACC', 'BVP', 'EDA', 'TEMP']\n",
    "        with open(os.path.join(main_path, self.name) + '/' + self.name + '.pkl', 'rb') as file:\n",
    "            self.data = pickle.load(file, encoding='latin1')\n",
    "        self.labels = self.data['label']\n",
    "\n",
    "    def get_wrist_data(self):\n",
    "        data = self.data['signal']['wrist']\n",
    "        data.update({'Resp': self.data['signal']['chest']['Resp']})\n",
    "        return data\n",
    "\n",
    "    def get_chest_data(self):\n",
    "        return self.data['signal']['chest']\n",
    "\n",
    "    def extract_features(self):  # only wrist\n",
    "        results = \\\n",
    "            {\n",
    "                key: get_statistics(self.get_wrist_data()[key].flatten(), self.labels, key)\n",
    "                for key in self.wrist_keys\n",
    "            }\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "269e4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ppg_data(e4_data_dict, labels, norm_type=None):\n",
    "    # Dataframes for each sensor type\n",
    "    df = pd.DataFrame(e4_data_dict['BVP'], columns=['BVP'])\n",
    "    label_df = pd.DataFrame(labels, columns=['label'])\n",
    "    \n",
    "\n",
    "    # Adding indices for combination due to differing sampling frequencies\n",
    "    df.index = [(1 / fs_dict['BVP']) * i for i in range(len(df))]\n",
    "    label_df.index = [(1 / fs_dict['label']) * i for i in range(len(label_df))]\n",
    "\n",
    "    # Change indices to datetime\n",
    "    df.index = pd.to_datetime(df.index, unit='s')\n",
    "    label_df.index = pd.to_datetime(label_df.index, unit='s')\n",
    "\n",
    "    df = df.join(label_df, how='outer')\n",
    "    \n",
    "    df['label'] = df['label'].fillna(method='bfill')\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    if norm_type == 'std':  # 시그널 자체를 normalization\n",
    "        # std norm\n",
    "        df['BVP'] = (df['BVP'] - df['BVP'].mean()) / df['BVP'].std()\n",
    "    elif norm_type == 'minmax':\n",
    "        # minmax norm\n",
    "        df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "    # Groupby\n",
    "    df = df.dropna(axis=0) # nan인 행 제거\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3169f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_data_by_label(df):\n",
    "    \n",
    "    grouped = df.groupby('label')\n",
    "    baseline = grouped.get_group(1)\n",
    "    stress = grouped.get_group(2)\n",
    "    amusement = grouped.get_group(3)\n",
    "    meditation = grouped.get_group(4)\n",
    "    \n",
    "    return grouped, baseline, stress, amusement, meditation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49d5d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(data, label, ma_usage):\n",
    "    global feat_names\n",
    "    global WINDOW_IN_SECONDS\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    window_len = fs_dict['BVP'] * WINDOW_IN_SECONDS  # 64*60 , sliding window: 0.25 sec (60*0.25 = 15)   \n",
    "    sliding_window_len = int(fs_dict['BVP'] * WINDOW_IN_SECONDS * 0.25)\n",
    "    \n",
    "    winNum = 0\n",
    "    \n",
    "    i = 0\n",
    "    while sliding_window_len * i <= len(data) - window_len:\n",
    "        \n",
    "         # 한 윈도우에 해당하는 모든 윈도우 담기,\n",
    "        w = data[sliding_window_len * i: (sliding_window_len * i) + window_len]  \n",
    "        # Calculate stats for window\n",
    "        wstats = get_window_stats_27_features(ppg_seg=w['BVP'].tolist(), window_length = window_len, label=label, ensemble = ENSEMBLE, ma_usage = ma_usage)\n",
    "        winNum += 1\n",
    "        \n",
    "        if wstats == []:\n",
    "            i += 1\n",
    "            continue;\n",
    "        # Seperating sample and label\n",
    "        x = pd.DataFrame(wstats, index = [i])\n",
    "    \n",
    "        samples.append(x)\n",
    "        i += 1\n",
    "\n",
    "    return pd.concat(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51a1bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(subjects):\n",
    "    df_list = []\n",
    "    for s in subjects:\n",
    "        df = pd.read_csv(f'{savePath}{subject_feature_path}/S{s}_feats_4.csv', index_col=0)\n",
    "        df['subject'] = s\n",
    "        df_list.append(df)\n",
    "\n",
    "    df = pd.concat(df_list)\n",
    "\n",
    "    df['label'] = (df['0'].astype(str) + df['1'].astype(str)).apply(lambda x: x.index('1'))  # 1인 부분의 인덱스 반환\n",
    "    df.drop(['0', '1'], axis=1, inplace=True)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df.to_csv(savePath + merged_path)\n",
    "\n",
    "    counts = df['label'].value_counts()\n",
    "    print('Number of samples per class:')\n",
    "    for label, number in zip(counts.index, counts.values):\n",
    "        print(f'{int_to_label[label]}: {number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f833c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_patient_data(subject_id, ma_usage):\n",
    "    global savePath\n",
    "    global WINDOW_IN_SECONDS\n",
    "\n",
    "    temp_ths = [1.0,2.0,1.8,1.5] #[1.1,2.2,2.0,1.9] #[1.1,2.2,1.8,1.9]\n",
    "    clean_df = pd.read_csv('clean_signal_by_rate.csv',index_col=0)\n",
    "    cycle = 15\n",
    "    \n",
    "    # Make subject data object for Sx\n",
    "    subject = SubjectData(main_path=main_path, subject_number=subject_id)\n",
    "    \n",
    "    # Empatica E4 data\n",
    "    e4_data_dict = subject.get_wrist_data()\n",
    "\n",
    "    # norm type\n",
    "    norm_type = 'std'\n",
    "\n",
    "    df = extract_ppg_data(e4_data_dict, subject.labels, norm_type)\n",
    "    df_BVP = df.BVP\n",
    "\n",
    "\n",
    "    #여기서 signal preprocessing \n",
    "    bp_bvp = butter_bandpassfilter(df_BVP, 0.5, 10, fs_dict['BVP'], order=2) # 0.5, 5 -> 0.5,10\n",
    "    \n",
    "    if BP:     \n",
    "        df['BVP'] = bp_bvp\n",
    "        \n",
    "    if FREQ:\n",
    "        signal_one_percent = int(len(df_BVP) * 0.01)\n",
    "        print(signal_one_percent)\n",
    "        cutoff = get_cutoff(df_BVP[:signal_one_percent], fs_dict['BVP'])\n",
    "        freq_signal = compute_and_reconstruction_dft(df_BVP, fs_dict['BVP'], sec, overlap, cutoff)\n",
    "        df['BVP'] = freq_signal\n",
    "\n",
    "    if TIME:\n",
    "        fwd = moving_average(bp_bvp, size=3)\n",
    "        bwd = moving_average(bp_bvp[::-1], size=3)\n",
    "        bp_bvp = np.mean(np.vstack((fwd,bwd[::-1])), axis=0)\n",
    "        df['BVP'] = bp_bvp\n",
    "        \n",
    "        signal_01_percent = int(len(df_BVP) * 0.001)\n",
    "        print(signal_01_percent, int(clean_df.loc[subject_id]['index']))\n",
    "        clean_signal = df_BVP[int(clean_df.loc[subject_id]['index']):int(clean_df.loc[subject_id]['index'])+signal_01_percent]\n",
    "        ths = statistic_threshold(clean_signal, fs_dict['BVP'], temp_ths)\n",
    "        len_before, len_after, time_signal_index = eliminate_noise_in_time(df['BVP'].to_numpy(), fs_dict['BVP'], ths, cycle)\n",
    "    \n",
    "        df = df.iloc[time_signal_index,:]\n",
    "        df = df.reset_index(drop=True)\n",
    "        #plt.figure(figsize=(40,20))\n",
    "        #plt.plot(df['BVP'][:2000], color = 'b', linewidth=2.5)\n",
    "    \n",
    "    \n",
    "    grouped, baseline, stress, amusement, meditation = seperate_data_by_label(df)   \n",
    "    \n",
    "    \n",
    "    \n",
    "    baseline_samples = get_samples(baseline, 1, ma_usage)\n",
    "    print(\"bas: \",len(baseline_samples))\n",
    "    stress_samples = get_samples(stress, 2, ma_usage)\n",
    "    print(\"st: \",len(stress_samples))\n",
    "    amusement_samples = get_samples(amusement, 0, ma_usage)\n",
    "    print(\"Am: \",len(amusement_samples))\n",
    "    meditation_samples = get_samples(meditation, 3, ma_usage)\n",
    "    print(\"Me: \",len(meditation_samples))\n",
    "    window_len = len(baseline_samples)+len(stress_samples)+len(amusement_samples)+len(meditation_samples)\n",
    "\n",
    "    all_samples = pd.concat([baseline_samples, stress_samples, amusement_samples, meditation_samples])\n",
    "    all_samples = pd.concat([all_samples.drop('label', axis=1), pd.get_dummies(all_samples['label'])], axis=1) # get dummies로 원핫벡터로 라벨값 나타냄\n",
    "    \n",
    "    \n",
    "    all_samples.to_csv(f'{savePath}{subject_feature_path}/S{subject_id}_feats_4.csv')\n",
    "\n",
    "    # Does this save any space?\n",
    "    subject = None\n",
    "    \n",
    "    return window_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1a3df77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for S2...\n",
      "389 1500\n",
      "389056 338602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bas:  32\n",
      "st:  14\n",
      "Am:  9\n",
      "Me:  21\n",
      "Processing data for S3...\n",
      "415 16000\n",
      "415552 368357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bas:  29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st:  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "A theoretically impossible result was found during the iteration\n",
      "process for finding a smoothing spline with fp = s: s too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Am:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me:  22\n",
      "Processing data for S4...\n",
      "411 31000\n",
      "411072 402110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bas:  35\n",
      "st:  18\n",
      "Am:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me:  23\n",
      "Processing data for S5...\n",
      "400 78400\n",
      "400512 361541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bas:  36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st:  8\n",
      "Am:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me:  23\n",
      "Processing data for S6...\n",
      "452 60000\n",
      "452544 422591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bas:  35\n",
      "st:  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Am:  8\n",
      "Me:  23\n",
      "Processing data for S7...\n",
      "335 105500\n",
      "335232 286387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bas:  34\n",
      "st:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Am:  9\n",
      "Me:  23\n",
      "Processing data for S8...\n",
      "349 24000\n",
      "349824 330072\n",
      "bas:  35\n",
      "st:  15\n",
      "Am:  9\n",
      "Me:  23\n",
      "Processing data for S9...\n",
      "334 231000\n",
      "334272 313986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bas:  36\n",
      "st:  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Am:  9\n",
      "Me:  23\n",
      "Processing data for S10...\n",
      "351 102000\n",
      "351744 332597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bas:  34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st:  20\n",
      "Am:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me:  23\n",
      "Processing data for S11...\n",
      "334 232000\n",
      "334912 306923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bas:  28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st:  18\n",
      "Am:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me:  23\n",
      "Processing data for S13...\n",
      "354 153000\n",
      "354368 344110\n",
      "bas:  35\n",
      "st:  15\n",
      "Am:  9\n",
      "Me:  23\n",
      "Processing data for S14...\n",
      "355 30000\n",
      "355072 350378\n",
      "bas:  36\n",
      "st:  19\n",
      "Am:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me:  23\n",
      "Processing data for S15...\n",
      "336 10000\n",
      "336128 326967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bas:  36\n",
      "st:  17\n",
      "Am:  9\n",
      "Me:  23\n",
      "Processing data for S16...\n",
      "360 257000\n",
      "360384 337646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bas:  33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st:  19\n",
      "Am:  8\n",
      "Me:  22\n",
      "Processing data for S17...\n",
      "378 219200\n",
      "378880 340639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bas:  34\n",
      "st:  18\n",
      "Am:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:313: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me:  21\n",
      "Number of samples per class:\n",
      "baseline: 508\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m     window_len \u001b[38;5;241m=\u001b[39m make_patient_data(patient, BP)\n\u001b[1;32m     55\u001b[0m     total_window_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m window_len\n\u001b[0;32m---> 57\u001b[0m \u001b[43mcombine_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_Window_len: \u001b[39m\u001b[38;5;124m'\u001b[39m,total_window_len)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing complete.\u001b[39m\u001b[38;5;124m'\u001b[39m, n)\n",
      "Cell \u001b[0;32mIn[44], line 20\u001b[0m, in \u001b[0;36mcombine_files\u001b[0;34m(subjects)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of samples per class:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, number \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(counts\u001b[38;5;241m.\u001b[39mindex, counts\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mint_to_label\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumber\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 3"
     ]
    }
   ],
   "source": [
    "def combine_files(subjects):\n",
    "    df_list = []\n",
    "    for s in subjects:\n",
    "        df = pd.read_csv(f'{savePath}{subject_feature_path}/S{s}_feats_4.csv', index_col=0)\n",
    "        df['subject'] = s\n",
    "        df_list.append(df)\n",
    "\n",
    "    df = pd.concat(df_list)\n",
    "\n",
    "    df['label'] = (df['0'].astype(str) + df['1'].astype(str) + df['2'].astype(str) + df['3'].astype(str)).apply(lambda x: x.index('1'))  # 1인 부분의 인덱스 반환\n",
    "    df.drop(['0', '1', '2', '3'], axis=1, inplace=True)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df.to_csv(savePath + merged_path)\n",
    "\n",
    "    counts = df['label'].value_counts()\n",
    "    print('Number of samples per class:')\n",
    "    for label, number in zip(counts.index, counts.values):\n",
    "        print(f'{int_to_label[label]}: {number}')\n",
    "\n",
    "\n",
    "# +\n",
    "total_window_len = 0\n",
    "BP, FREQ, TIME, ENSEMBLE = False, False, False, False\n",
    "subject_ids = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]\n",
    "\n",
    "feat_names = None\n",
    "savePath = '27_features_ppg_test_4/LMM'\n",
    "\n",
    "if not os.path.exists(savePath):\n",
    "    os.makedirs(savePath)\n",
    "\n",
    "\n",
    "for n in NOISE:\n",
    "    if 'bp' in n.split('_'):\n",
    "        BP = True\n",
    "    if 'time' in n.split('_'):\n",
    "        TIME = True\n",
    "    if 'ens' in n.split('_'):\n",
    "        ENSEMBLE = True\n",
    "        \n",
    "\n",
    "\n",
    "    subject_feature_path = '/subject_feature_' + n + str(WINDOW_IN_SECONDS)\n",
    "    merged_path = '/data_merged_' + n + str(WINDOW_IN_SECONDS) +'.csv'\n",
    "    \n",
    "    if not os.path.exists(savePath + subject_feature_path):\n",
    "        os.makedirs(savePath + subject_feature_path)\n",
    "    \n",
    "        \n",
    "    for patient in subject_ids:\n",
    "        print(f'Processing data for S{patient}...')\n",
    "        window_len = make_patient_data(patient, BP)\n",
    "        total_window_len += window_len\n",
    "\n",
    "    combine_files(subject_ids)\n",
    "    print('total_Window_len: ',total_window_len)\n",
    "    print('Processing complete.', n)\n",
    "    total_window_len = 0\n",
    "# -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (NGC 24.01 / TensorFlow 2.14) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
