{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776a05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef28a040",
   "metadata": {},
   "source": [
    "## To Label the Data with 3rd class (Pre-FOG)\n",
    "\n",
    "Before the occurence of every FOG event, the previous w*f_s timesteps are labelled as a third class 'preFOG' which can be trained in order to predict FOG before it's onset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5a1c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_prefog(dataset, window_length=1):\n",
    "    dataset.drop(index=list(dataset[dataset['Action'] == 0].index), inplace=True)\n",
    "    window_length = 64 * window_length\n",
    "\n",
    "    fog_index = []\n",
    "    for i in dataset.index:\n",
    "        if dataset.loc[i, 'Action'] == 2:\n",
    "            fog_index.append(i)\n",
    "\n",
    "    start_indices = []\n",
    "    for i in fog_index:\n",
    "        if dataset.loc[i-1, 'Action'] != dataset.loc[i, 'Action']:\n",
    "            start_indices.append(i)\n",
    "\n",
    "    prefog = []\n",
    "    for start in start_indices:\n",
    "        prefog_start = [x for x in range(start-window_length, start)]\n",
    "        prefog.append(prefog_start)\n",
    "\n",
    "    prefog = [item for sublist in prefog for item in sublist]\n",
    "\n",
    "    for i in prefog:\n",
    "        dataset.loc[i, 'Action'] = 3\n",
    "    dataset['Action'] = dataset['Action'] - 1\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e5823f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R02.txt  is read\tAdding S07R02.txt to dataset\tS07R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R01.txt  is read\tAdding S07R01.txt to dataset\tS07R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\t\n",
      "S10\n",
      "S10R01.txt  is read\t\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R02.txt  is read\tAdding S07R02.txt to dataset\tS07R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R01.txt  is read\tAdding S07R01.txt to dataset\tS07R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\t\n",
      "S10\n",
      "S10R01.txt  is read\t\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R02.txt  is read\tAdding S07R02.txt to dataset\tS07R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R01.txt  is read\tAdding S07R01.txt to dataset\tS07R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\t\n",
      "S10\n",
      "S10R01.txt  is read\t\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R01.txt  is read\tAdding S01R01.txt to dataset\tS01R01.txt is labelled\n",
      "\n",
      "S01\n",
      "S01R02.txt  is read\tAdding S01R02.txt to dataset\tS01R02.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R01.txt  is read\tAdding S02R01.txt to dataset\tS02R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R01.txt  is read\tAdding S03R01.txt to dataset\tS03R01.txt is labelled\n",
      "\n",
      "S02\n",
      "S02R02.txt  is read\tAdding S02R02.txt to dataset\tS02R02.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R02.txt  is read\tAdding S03R02.txt to dataset\tS03R02.txt is labelled\n",
      "\n",
      "S05\n",
      "S05R01.txt  is read\tAdding S05R01.txt to dataset\tS05R01.txt is labelled\n",
      "\n",
      "S03\n",
      "S03R03.txt  is read\t\n",
      "S04\n",
      "S04R01.txt  is read\t\n",
      "S05\n",
      "S05R02.txt  is read\tAdding S05R02.txt to dataset\tS05R02.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R02.txt  is read\tAdding S07R02.txt to dataset\tS07R02.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R01.txt  is read\tAdding S06R01.txt to dataset\tS06R01.txt is labelled\n",
      "\n",
      "S07\n",
      "S07R01.txt  is read\tAdding S07R01.txt to dataset\tS07R01.txt is labelled\n",
      "\n",
      "S06\n",
      "S06R02.txt  is read\t\n",
      "S10\n",
      "S10R01.txt  is read\t\n",
      "S09\n",
      "S09R01.txt  is read\tAdding S09R01.txt to dataset\tS09R01.txt is labelled\n",
      "\n",
      "S08\n",
      "S08R01.txt  is read\tAdding S08R01.txt to dataset\tS08R01.txt is labelled\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = \"dataset/\"\n",
    "\n",
    "people = []\n",
    "for person in os.listdir(data_path):\n",
    "    if '.txt' in person:\n",
    "        people.append(person)\n",
    "\n",
    "for window_length in range(1, 5):\n",
    "    dataset = pd.DataFrame()  # 매 루프마다 dataset을 초기화\n",
    "    for person in people:\n",
    "        name = person.split('R')[0]\n",
    "        print(name)\n",
    "        file = data_path + person\n",
    "        temp = pd.read_csv(file, delimiter=\" \", header=None)\n",
    "        print(person, ' is read', end='\\t')\n",
    "        if 2 in temp[max(temp.columns)].unique():\n",
    "            print('Adding {} to dataset'.format(person), end='\\t')\n",
    "            temp.columns = ['time', 'A_F', 'A_V', 'A_L', 'L_F', 'L_V', 'L_L', 'T_F', 'T_V', 'T_L', 'Action']\n",
    "            temp = label_prefog(temp, window_length).reset_index(drop=True)\n",
    "            temp['name'] = name\n",
    "            print('{} is labelled'.format(person))\n",
    "            dataset = pd.concat([dataset, temp], axis=0)\n",
    "\n",
    "        print('')\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    to_path = data_path + \"raw_labelled\"\n",
    "    os.makedirs(to_path, exist_ok=True)  # 디렉토리가 없으면 생성합니다.\n",
    "    to_name = to_path + f\"/win_{window_length}.csv\"\n",
    "    dataset.to_csv(to_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e7a8d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     time  A_F   A_V  A_L  L_F  L_V  L_L  T_F   T_V  T_L  Action name\n",
      "0  750000  -30   990  326  -45  972  181  -38  1000   29       0  S01\n",
      "1  750015  -30  1000  356  -18  981  212  -48  1028   29       0  S01\n",
      "2  750031  -20   990  336   18  981  222  -38  1038    9       0  S01\n",
      "3  750046  -20  1000  316   36  990  222  -19  1038    9       0  S01\n",
      "4  750062    0   990  316   36  990  212  -29  1038   29       0  S01\n"
     ]
    }
   ],
   "source": [
    "# 추가된 부분\n",
    "# 이 부분에서 path 변수를 정의합니다.\n",
    "path = os.getcwd() + \"/dataset\"\n",
    "\n",
    "col = ['A_F', 'A_V', 'A_L', 'L_F', 'L_V', 'L_L', 'T_F', 'T_V', 'T_L']\n",
    "stat1 = pd.DataFrame(columns=col)  # 빈 데이터프레임을 초기화할 때 컬럼을 설정합니다.\n",
    "features_path = path + \"/features\"\n",
    "os.makedirs(features_path, exist_ok=True)  # 디렉토리가 없으면 생성합니다.\n",
    "feature_name = features_path + f\"/time_{window_length}.csv\"\n",
    "stat1.to_csv(feature_name, index=False)\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ada580",
   "metadata": {},
   "source": [
    "## To Extract Non-Overlapping windows of length w *f_s from the continously logged accelerometer data from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22d92989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(act,window_length,dataframe):\n",
    "    \n",
    "  indices = list(dataframe[dataframe.Action == act].index)\n",
    "  groups = []\n",
    "  temp = []\n",
    "  group_count = 0\n",
    "  for i in range(len(indices)):\n",
    "    if i == len(indices)-1:\n",
    "      temp.append(indices[i])\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "      break\n",
    "    temp.append(indices[i])\n",
    "    if indices[i]+1 != indices[i+1]: \n",
    "      group_count+=1\n",
    "      groups.append(temp)\n",
    "      temp = []\n",
    "\n",
    "  fs = 64\n",
    "  window_length = 1\n",
    "  # window_length = window_length*fs\n",
    "\n",
    "  final_dataframe = pd.DataFrame()\n",
    "  for i in groups: \n",
    "    required = math.floor(len(i)/(window_length*fs))\n",
    "    \n",
    "    req_index = i[0:(required*fs)]\n",
    "    \n",
    "    final_dataframe = pd.concat([final_dataframe,dataframe.iloc[req_index,:]],axis = 0)\n",
    "  return final_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf0d6ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for window_length in range(1,5):\n",
    "  \n",
    "  path = os.getcwd()+\"/dataset\"\n",
    "  name = path + f\"/raw_labelled/win_{window_length}.csv\"\n",
    "  dataframe = pd.read_csv(name)\n",
    "\n",
    "  activities = []\n",
    "  for act in range(3):\n",
    "    activities.append(create_window(act,window_length,dataframe))\n",
    "  to_write = pd.concat(activities, axis=0)\n",
    "\n",
    "  windows_path = path + \"/windows\"\n",
    "  os.makedirs(windows_path, exist_ok=True)  # 디렉토리가 없으면 생성합니다.\n",
    "  to_path = windows_path + f\"/windowed_{window_length}.csv\"\n",
    "  to_write.to_csv(to_path, index=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad95ae",
   "metadata": {},
   "source": [
    "## Extracting Features\n",
    "\n",
    "The following feature are extracted in the time domain\n",
    "\n",
    "1. Mean\n",
    "2. std\n",
    "3. var\n",
    "4. Mav\n",
    "5. rms\n",
    "\n",
    "The following feature are extracted in the frequency domain\n",
    "\n",
    "1. Freeze Index\n",
    "2. Power\n",
    "3. Energy\n",
    "4. Entropy\n",
    "5. Peak Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07895780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_F\n",
      "A_V\n",
      "A_L\n",
      "L_F\n",
      "L_V\n",
      "L_L\n",
      "T_F\n",
      "T_V\n",
      "T_L\n"
     ]
    }
   ],
   "source": [
    "#read file \n",
    "window_length = 1\n",
    "fs = 64\n",
    "# for window_length in range(1,5):\n",
    "w = window_length*fs\n",
    "FE_path = path + \"/windows/windowed_\"\n",
    "name = FE_path + str(window_length) + \".csv\"\n",
    "dataframe = pd.read_csv(name)\n",
    "\n",
    "df = dataframe.drop(columns=['time','Action','name'])\n",
    "stat = pd.DataFrame()\n",
    "\n",
    "\n",
    "col= list(df.columns)\n",
    "for s in col:    \n",
    "  print (s)\n",
    "  mn =[] \n",
    "  var = []\n",
    "  std = []\n",
    "  mav = []\n",
    "  rms =[]\n",
    "  for i in range(0,len(df),w):\n",
    "      mn_  = np.mean(df[s].iloc[i:i+w])\n",
    "      var_  = np.var(df[s].iloc[i:i+w])\n",
    "      std_  = np.std(df[s].iloc[i:i+w])\n",
    "      mav_  = np.mean(abs(df[s].iloc[i:i+w]))\n",
    "      rms_  = np.sqrt(np.mean((df[s].iloc[i:i+w])**2))\n",
    "\n",
    "      mn.append(mn_)\n",
    "      var.append(var_)\n",
    "      std.append(std_)\n",
    "      mav.append(mav_)\n",
    "      rms.append(rms_)\n",
    "\n",
    "  stat['mean_'+s] = mn\n",
    "  stat['var_'+s] = var\n",
    "  stat['std_'+s] = std\n",
    "  stat['rms_'+s] = rms\n",
    "  stat['mav_'+s] = mav\n",
    "\n",
    "\n",
    "stat.shape\n",
    "\n",
    "\n",
    "import copy\n",
    "stat1 = copy.copy(stat)\n",
    "stat1['w'] = dataframe['Action'].iloc[[x for x in range(0,len(dataframe),w)]].to_list()\n",
    "order = ['w']\n",
    "order += stat1.columns.to_list()[:-1]\n",
    "stat1 = stat1[order]\n",
    "stat1.columns\n",
    "col = stat1.columns.to_list()\n",
    "col[0] = 0\n",
    "stat1.columns = col\n",
    "feature_name = path + \"/features/time_\"+str(window_length)+\".csv\"\n",
    "stat1.to_csv(feature_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bf421c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze and power done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_394/2963688091.py:76: RuntimeWarning: invalid value encountered in divide\n",
      "  p_norm = Pxx_den/sum(Pxx_den)\n",
      "/tmp/ipykernel_394/2963688091.py:76: RuntimeWarning: invalid value encountered in divide\n",
      "  p_norm = Pxx_den/sum(Pxx_den)\n",
      "/tmp/ipykernel_394/2963688091.py:76: RuntimeWarning: invalid value encountered in divide\n",
      "  p_norm = Pxx_den/sum(Pxx_den)\n"
     ]
    }
   ],
   "source": [
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "window_length = 3\n",
    "fs = 64\n",
    "# for window_length in range(1,5):\n",
    "w = window_length*fs\n",
    "FE_path = path + \"/windows/windowed_\"\n",
    "name = FE_path + str(window_length) + \".csv\"\n",
    "dataframe = pd.read_csv(name)\n",
    "\n",
    "df = dataframe.drop(columns=['time','Action','name'])\n",
    "\n",
    "col= list(df.columns)\n",
    "\n",
    "order=5\n",
    "\n",
    "fi=pd.DataFrame()\n",
    "\n",
    "power = pd.DataFrame()\n",
    "bands = {'locomotor' :(0.5,3),'freeze' :(3,8)}\n",
    "\n",
    "for s in col:\n",
    "    xtemp = []\n",
    "    xtemp1 = []\n",
    "    for i in range(0,len(df),w):\n",
    "        nyq=0.5*fs\n",
    "        \n",
    "        #locomotor band 0.5-3hz\n",
    "        loc_low= 0.5/nyq\n",
    "        loc_high=3/nyq\n",
    "        \n",
    "        #clipping off band from the window\n",
    "        b, a = butter(order, [loc_low, loc_high], btype='band')\n",
    "        y=lfilter(b,a,df[s].iloc[i:i+w])\n",
    "        \n",
    "        #total power in locomotor band\n",
    "        e1=sum([x**2 for x in y])\n",
    "\n",
    "        #freeze band 3-8hz\n",
    "        frez_low= 3/nyq\n",
    "        frez_high=8/nyq\n",
    "\n",
    "        #clipping off band from the window\n",
    "        b1, a1 = butter(order, [frez_low, frez_high], btype='band')\n",
    "        y1=lfilter(b1,a1,df[s].iloc[i:i+w])\n",
    "        #total power in locomotor band\n",
    "        e2=sum([x**2 for x in y1])\n",
    "        \n",
    "        FI=e2/e1\n",
    "        POW=e2+e1\n",
    "        xtemp.append(FI)\n",
    "        xtemp1.append(POW)\n",
    "    fi['FI'+s] = xtemp\n",
    "    power['P'+s] = xtemp1\n",
    "print (\"Freeze and power done\")\n",
    "\n",
    "\n",
    "w = window_length*fs\n",
    "E=[]\n",
    "for i in range(0,len(df),w):\n",
    "  energy = np.sum((df.iloc[i:i+w,:])**2)\n",
    "  E.append(energy)\n",
    "E = pd.DataFrame(E)\n",
    "E.columns = [\"EN_\" + x for x in df.columns]\n",
    "\n",
    "#Entropy \n",
    "from scipy.signal import periodogram\n",
    "\n",
    "peak_f = pd.DataFrame()\n",
    "PSE = pd.DataFrame()\n",
    "for s in col:\n",
    "  peakF = []\n",
    "  pse = []\n",
    "  for i in range(0,len(df),w):\n",
    "      f,Pxx_den = periodogram(df[s].iloc[i:i+w],fs)\n",
    "      p_norm = Pxx_den/sum(Pxx_den)\n",
    "      p_norm = list(filter(lambda a: a != 0, p_norm))\n",
    "      pse.append(-(np.sum(p_norm*np.log(p_norm))))\n",
    "      peak = (fs/w)*max(Pxx_den)\n",
    "      peakF.append(peak)\n",
    "  PSE['ENt_'+s] = pse\n",
    "  peak_f['peak_'+s] = peakF\n",
    "PSE.fillna(0,inplace = True)\n",
    "\n",
    "\n",
    "freq = pd.concat([fi,power,E,PSE,peak_f],axis = 1)\n",
    "\n",
    "feature_name = path + \"/features/freq_\"+str(window_length)+\".csv\"\n",
    "freq.to_csv(feature_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fdb69c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(811200, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37e6fb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.2475133510467163e-32,\n",
       " 0.008737503700812927,\n",
       " 0.057239190898979796,\n",
       " 0.31419744338747646,\n",
       " 0.05723153572091037,\n",
       " 0.040425336591763464,\n",
       " 0.0038633418321145758,\n",
       " 0.04035895812284892,\n",
       " 0.004548671813727431,\n",
       " 0.006411827107515323,\n",
       " 0.021938571733910717,\n",
       " 0.020931521571531754,\n",
       " 0.0060894162608230865,\n",
       " 0.004171999271817693,\n",
       " 0.006813729308807736,\n",
       " 0.033312735814741444,\n",
       " 0.042450475769469226,\n",
       " 0.004005058772182705,\n",
       " 0.012051711376991783,\n",
       " 0.0004617421185715281,\n",
       " 0.00830289452004817,\n",
       " 0.0004641823826224023,\n",
       " 0.022746701171254633,\n",
       " 0.01582728154476885,\n",
       " 0.01648019071179349,\n",
       " 0.003891162537965589,\n",
       " 0.001655848263382903,\n",
       " 0.004423960163810015,\n",
       " 0.014612890740322921,\n",
       " 0.000665443246263766,\n",
       " 0.0027637184722803486,\n",
       " 0.0014418740542361363,\n",
       " 0.002838284050597186,\n",
       " 0.006221949506275692,\n",
       " 0.007616654253299707,\n",
       " 0.007245866275830395,\n",
       " 0.0024514351938327017,\n",
       " 0.0029871879218501592,\n",
       " 0.0012231884121725212,\n",
       " 0.0023006624143964015,\n",
       " 0.003825775594873588,\n",
       " 0.00044759689008329106,\n",
       " 0.0016845887591069573,\n",
       " 0.0007532396379237761,\n",
       " 0.00022732307502422282,\n",
       " 0.0016929462158357031,\n",
       " 0.0003087551644028866,\n",
       " 0.004754543660118336,\n",
       " 0.0032052867866972,\n",
       " 0.0021969032985349694,\n",
       " 0.004286118237962164,\n",
       " 0.0009134819977665407,\n",
       " 0.0067860630531853365,\n",
       " 0.018383097238428105,\n",
       " 0.0018830687088007675,\n",
       " 0.0016455536474346084,\n",
       " 0.0011548743370073476,\n",
       " 0.000787602654683932,\n",
       " 0.008184495363919528,\n",
       " 0.006988210964314714,\n",
       " 0.0005859297888641,\n",
       " 0.0039502690652272986,\n",
       " 0.006149477613986835,\n",
       " 0.0005752830501450821,\n",
       " 0.0013522527536222548,\n",
       " 0.006092807280384314,\n",
       " 3.698833832649189e-05,\n",
       " 0.007505951864048572,\n",
       " 0.004530760617517066,\n",
       " 0.010055108088639101,\n",
       " 0.0012178799667439556,\n",
       " 0.0010712833514689253,\n",
       " 0.005158797848217318,\n",
       " 0.0015985095359581048,\n",
       " 0.006033022655962021,\n",
       " 0.0011251168789940323,\n",
       " 0.0064768303004568714,\n",
       " 0.002349260352207662,\n",
       " 0.002274260145726851,\n",
       " 0.002824323627281735,\n",
       " 0.0008484662051819731,\n",
       " 0.004265103959538152,\n",
       " 0.0014317491386374888,\n",
       " 0.0007139932090122746,\n",
       " 0.0030625935424086798,\n",
       " 0.0006333213857227973,\n",
       " 0.006411217527557521,\n",
       " 0.001723972094249808,\n",
       " 0.00919763280234142,\n",
       " 0.0018692696354978416,\n",
       " 0.0012780900752384777,\n",
       " 0.003621192262083396,\n",
       " 0.00453326395922554,\n",
       " 0.0025230060850544344,\n",
       " 0.0013877059621357605,\n",
       " 0.0034535713214308324,\n",
       " 0.0005700654128001962]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (NGC 24.01 / TensorFlow 2.14) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
