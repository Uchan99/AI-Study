{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"183WP29m-Y20-CQFyAwdLwvfi0I-7UGhH","authorship_tag":"ABX9TyPIDyTOhWj8Gr/HcH0VywpW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**07-2 심층 신경망**"],"metadata":{"id":"PLnRXF0dME33"}},{"cell_type":"code","source":["# 실행마다 동일한 결과를 얻기 위해 케라스에 랜덤 시드를 사용하고 텐서플로 연산을 결정적으로 만듭니다.\n","import tensorflow as tf\n","\n","tf.keras.utils.set_random_seed(42)\n","tf.config.experimental.enable_op_determinism()"],"metadata":{"id":"tyqu3q_bMU-B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 케라스 API를 사용해서 패션 MNIST 데이터셋 불러오기\n","\n","from tensorflow import keras\n","\n","(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9AawdkKMMZN0","executionInfo":{"status":"ok","timestamp":1705222752874,"user_tz":-540,"elapsed":775,"user":{"displayName":"유찬","userId":"05761325302258896347"}},"outputId":"1d371979-a0c9-4432-c5e5-d489e3999fa6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","29515/29515 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26421880/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","5148/5148 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4422102/4422102 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# 픽셀값 0~255에서 0~1 사이로 변경\n","train_scaled = train_input / 255.0\n","\n","# 2차원배열을 1차원 배열로 변경\n","train_scaled = train_scaled.reshape(-1, 28*28)\n","\n","train_scaled, val_scaled, train_target, val_target = train_test_split(\n","    train_scaled, train_target, test_size=0.2, random_state=42)"],"metadata":{"id":"iA6A5gGuMkQ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<img src= \"https://drive.google.com/uc?export=view&id=1QQYPoqWbNfUrcoorD9bsmqI_2a05wUR5\" widrh=300>\n","\n","입력층과 출력층 사이에 밀집층이 추가됨\n","- **입력층과 출력층 사이에 있는 모든 층을 은닉층(hidden layer)이라고 부름**\n","\n","은닉층에는 주황색 원으로 활성화 함수 표시\n","- **활성화 함수는 신경망 층의 선형 방정식의 계산 값에 적용하는 함수**\n","- 소프트맥스 함수도 활성화 함수 임\n","- 출력층에 적용하는 활성화 함수는 종류가 제한되어 있음\n"," - 이진 분류의 경우 시그모이드, 다중 분류는 소프트맥스\n"," - 은닉층의 활성화 함수는 비교적 자유로움\n","  - 대표적으로 시그뫼드 함수와 볼 렐루(LeRU) 함수 등"],"metadata":{"id":"L3Ow_-cIPfoO"}},{"cell_type":"markdown","source":["은닉층에 활성화 함수를 적용하는 이유?\n","\n","a x 4 + 2 = b <br>\n","b x 3 - 5 = c <br>\n","첫 번째 식에서 계산된 b가 두 번째 식에서 c를 계산하기 위해 사용됨\n","하지만 두 번째 식에 첫 번째 식을 대입하면 <br>\n","a x 12 + 1 = c <br>\n","이렇게 b가 사라지고 하나로 합쳐짐\n","\n","<br>\n","\n","신경망에서도 역시 은닉층에서 선형적인 산술 계산만 수행하면 수행 역할이 없는 셈이 되므로 선형 계산을 적당하게 비선형적으로 비틀어 주어야 다음 층의 계산과 단순히 합쳐지지 않고 나름의 역할을 할 수 있음\n","\n","a x 4 = b <br>\n","log(b) = k <br>\n","k x 3 - 5 = c"],"metadata":{"id":"2j_D273AQlyI"}},{"cell_type":"markdown","source":["시그모이드 함수는 뉴런의 출력 z 값을 0과 1 사이로 압축"],"metadata":{"id":"S-MoRJ_LUuu6"}},{"cell_type":"code","source":["# 시그모이드 활성화 함수를 사용한 은닉층과 소프트 맥스 함수를 사용한 출력층을 케라스의 Dense 클래스로 생성\n","# 케라스에서 신경망의 첫 번째 층은 input_shape 매개변수로 입력의 크기를 지정해 줘야만 함\n","\n","dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))\n","dense2 = keras.layers.Dense(10, activation='softmax')"],"metadata":{"id":"8-S2rthPM98c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["dense1\n","- 은닉층이고 100개의 뉴런을 가진 밀집 층\n","- 활성화 함수를 'sigmoid'로 지정\n","- 입력의 크기를 (784,)로 지정\n","- 은닉층의 뉴런 개수를 정하는 데에는 기준 X\n"," - 적어도 출력층의 뉴런 보다는 많아야 함\n","\n","dense 2\n","- 10개의 클래스를 분류함로 10개의 뉴런\n","- 활성화 함수는 소프트 맥스 함수"],"metadata":{"id":"3JRKOokRVfFt"}},{"cell_type":"markdown","source":["# 심층 신경망 만들기\n"],"metadata":{"id":"js8OtexQWEMn"}},{"cell_type":"code","source":["# dense1과 dense2 객체를 Sequential 클래스에 추가해 심층 신경망(deep neaural) 생성\n","\n","model = keras.Sequential([dense1, dense2])"],"metadata":{"id":"qQvvXmNIYM60"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<img src= \"https://drive.google.com/uc?export=view&id=16OjAA0IjARGH8AAW8sYTaCBMGB7EMTyD\" widrh=300>\n","\n","**Sequential 클래스의 객체를 만들 때 여러 개의 층을 추가하려면 이처럼 dense1과 dense2를 리스트로 만들어 전달**\n","- 주의할 것은 출력층을 가장 마지막에 두어야 함\n","- 이 리스트는 가장 처음 등장하는 은닉층에서 마지막 출력층의 순서로 나열해야 함\n","\n","**인공 신경망의 강력한 성능은 이렇게 층을 추가하여 입력 데이터에 대해 연속적인 학습을 진행하는 능력에서 나옴**\n","- 2개 이상의 층을 추가할 수도 있음"],"metadata":{"id":"CfhNeqiJYSI0"}},{"cell_type":"code","source":["# 케라스는 모델의 summary() 메서드를 호출하면 층에 대한 유용한 정보를 얻을 수 있음\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-1JmxxT9VcYw","executionInfo":{"status":"ok","timestamp":1705226009625,"user_tz":-540,"elapsed":437,"user":{"displayName":"유찬","userId":"05761325302258896347"}},"outputId":"f895004d-2391-40ff-c87b-9091aeee6c16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense (Dense)               (None, 100)               78500     \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                1010      \n","                                                                 \n","=================================================================\n","Total params: 79510 (310.59 KB)\n","Trainable params: 79510 (310.59 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["1. 맨 첫 줄에 모델의 이름\n","2. 그 다음 이 모델에 들어 있는 층이 순서대로 나열\n","- 맨 처음 추가한 은닉층에서 출력층의 순서대로\n","- 층마다 층 이름, 클래스, 출력 크기, 모델 파라미터 개수가 출력됨\n"," - 층을 만들 때 name 매개변수로 이름 지정 가능, 지정x 시 자동으로 'dense'\n","\n"," - 출력 크기가 (None, 100)임\n","   - 첫 번째 차원은 샘플의 개수, 샘플 개수가 정의되지 않기 때문에 None\n","   - **케라스 모델의 fit() 메서드에 훈련 데이터를 주입하면 이 데이터를 한 번에 모두 사용하지 않고 잘게 나누어 여러 번에 걸쳐 경사 하강법 단계 수행(미니 배치 경사 하강법)**\n","   - 케라스의 기본 미니 배치는 32개, 이 값은 fit() 메서드에서 batch_size 매개변수로 바꿀 수 있음\n","   - 따라서 샘플 개수를 고정하지 않고 어떤 배치 크기에도 유연하게 대응할 수 있도록 None으로 지정\n","   - **이렇게 신경망 층에 입력되거나 출력되는 배열의 첫 번째 차원을 배치 차원이라고 부름**\n","   <br>\n","\n","   -  두 번째 100은 은닉층의 뉴런 개수를 100개로 두어 100개의 출력이 나옴, 즉 샘플마다 784개의 픽셀값이 은닉층을 통과하면서 100개의 특성으로 압축\n","  \n","  - 마지막으로 모델 파라미터 개수 출력\n","   - 이 층은 Dense 층이므로 입력 픽셀 784개와 100개의 모든 조합에 대한 가중치가 있음\n","   - 뉴런마다 1개의 절편이 있음\n","    <img src= \"https://drive.google.com/uc?export=view&id=1uYsoqYSSJWbf83j58Yuqu4KiNtw1psUr\" widrh=300>\n","\n","   - 두 번째 층의 출력 크기는 (None, 10)임\n","    - 배치 차원은 동일하게 None이고 출력 뉴런 개수가 10개이기 때문\n","    - 이 층의 모델 파라미터 개수는 100개의 은닉층 뉴런가 10개의 출력층 뉴런이 모두 연결되고 출력층의 뉴런마다 하나의 절편이 존재하므로 1010개\n","   <img src= \"https://drive.google.com/uc?export=view&id=1SXxatHDQltiT-NFGvdMEPSUSTNqw4AAB\" widrh=300>\n","\n","- summary() 메서드의 마지막에는 총 모델 파라미터 개수와 훈련되는 파라미터 개수가 동일하게 79510개로 나옴\n","  - 은닉층과 출력층의 파라미터 개수를 합친 것\n","  - 그 아래 훈련되지 않은 파라미터는 0으로 나옴\n","   - 간혹 경사 하강법으로 훈련되지 않는 파라미터를 가진 층이 있는데, 그 개수가 나옴"],"metadata":{"id":"PHo15InrZER4"}},{"cell_type":"markdown","source":["# 층을 추가하는 다른 방법\n","\n","**Sequential 클래스의 생성자 안에서 바로 Dense 클래스의 객체를 만드는 경우가 많음**"],"metadata":{"id":"GTQVArbjdI_G"}},{"cell_type":"code","source":["model = keras.Sequential([\n","    keras.layers.Dense(100, activation='sigmoid', input_shape=(784,), name='hidden'),\n","    keras.layers.Dense(10, activation='softmax', name='output')\n","], name='패션 MNIST 모델')"],"metadata":{"id":"cn3-05d1Y_dj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["특징\n","\n","- 이렇게 작업시 추가되는 층을 한 눈에 쉽게 알아볼 수 있음\n","- 모델의 이름과 달리 층의 이름은 반드시 영문이어야만 함\n","- 여러 모델과 많은 층을 사용할 경우 name 매개변수를 사용하면 구분이 쉬움\n","- 아주 많은 층을 추가하려면 Sequential 클래스 생성자가 매우 길어지는 단점\n","- 조건에 따라 층을 추가할 수도 없음"],"metadata":{"id":"1TrlAthWdemT"}},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JR75YzXIdZt5","executionInfo":{"status":"ok","timestamp":1705227229176,"user_tz":-540,"elapsed":19,"user":{"displayName":"유찬","userId":"05761325302258896347"}},"outputId":"b96f26fa-747f-462a-dd34-5b3b6fe7a30b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"패션 MNIST 모델\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," hidden (Dense)              (None, 100)               78500     \n","                                                                 \n"," output (Dense)              (None, 10)                1010      \n","                                                                 \n","=================================================================\n","Total params: 79510 (310.59 KB)\n","Trainable params: 79510 (310.59 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["Sequential 클래스에서 층을 추가할 때 가장 널리 사용하는 방법은 모델의 <font color =\"red\">add() 메서드</font>\n","- Sequential 클래스의 객체를 만들고 이 객체의 add() 메서드를 호출하여 층을 추가\n","- 한 눈에 추가되는 층을 볼 수 있고 프로그램 실행 시 동적으로 층을 선택하여 추가할 수 있음"],"metadata":{"id":"HcIOVbzWecbj"}},{"cell_type":"code","source":["model = keras.Sequential()\n","model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)))\n","model.add(keras.layers.Dense(10, activation='softmax'))"],"metadata":{"id":"YhLD9o2cdpTz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UIFxeVqkf823","executionInfo":{"status":"ok","timestamp":1705228157190,"user_tz":-540,"elapsed":13,"user":{"displayName":"유찬","userId":"05761325302258896347"}},"outputId":"e57dc2c9-fb59-4102-c6e5-b2938d585d42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_2 (Dense)             (None, 100)               78500     \n","                                                                 \n"," dense_3 (Dense)             (None, 10)                1010      \n","                                                                 \n","=================================================================\n","Total params: 79510 (310.59 KB)\n","Trainable params: 79510 (310.59 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# 5번의 에포크동안 모델 훈련\n","\n","model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')\n","model.fit(train_scaled, train_target, epochs = 5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cNhuLdDchL1q","executionInfo":{"status":"ok","timestamp":1705228359381,"user_tz":-540,"elapsed":24368,"user":{"displayName":"유찬","userId":"05761325302258896347"}},"outputId":"a6729c02-3a4c-4ff5-be10-e96ecfd55b07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1500/1500 [==============================] - 5s 3ms/step - loss: 0.5710 - accuracy: 0.8064\n","Epoch 2/5\n","1500/1500 [==============================] - 5s 4ms/step - loss: 0.4132 - accuracy: 0.8509\n","Epoch 3/5\n","1500/1500 [==============================] - 5s 3ms/step - loss: 0.3776 - accuracy: 0.8646\n","Epoch 4/5\n","1500/1500 [==============================] - 4s 3ms/step - loss: 0.3530 - accuracy: 0.8732\n","Epoch 5/5\n","1500/1500 [==============================] - 4s 3ms/step - loss: 0.3344 - accuracy: 0.8782\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7ad2de095300>"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["추가된 층이 선응을 향상 시킴\n","- 인공 신경망에 몇개의 층을 추가하더라도 compile()과 fit()의 사용법은 동일"],"metadata":{"id":"BAfroY5Ri6BX"}},{"cell_type":"markdown","source":["# 렐루 함수\n","\n","렐루함수(ReLU)\n","- 시그모이드 함수는 오른쪽과 왼쪽 끝으로 갈 수록 그래프가 누워있기 때문에 올바른 출력을 만드는데 신속한 대응이 어려움\n","- 특히 층이 많을수록 학습을 더 어렵게 함\n","- 이를 개선하기 위해 나온 다른 종류의 활성화 함수가 렐루함수\n","- 렐루 함수는 입력이 양수일 경우 마치 활성화 함수가 없는 것 처럼 그냥 입력을 통과 시키고 음수일 경우는 0으로 만듬\n","   <img src= \"https://drive.google.com/uc?export=view&id=1fc_Vj-HrVH7vjUcQPWndeubmL1J31uSP\" widrh=300>\n","\n"],"metadata":{"id":"H5uR2oHDjFx_"}},{"cell_type":"markdown","source":["렐루 함수는 max(z, 0)와 같이 쓸 수 있음\n","- 이 함수는 z가 0보다 크면 z를 출력하고 z가 0보다 작으면 0을 출력함\n","\n","**렐루 함수는 특히 이미지 처리에서 좋은 성능을 낸다고 알려짐**"],"metadata":{"id":"vljwD4dmn8rw"}},{"cell_type":"markdown","source":["패션 MNIST 데이터는 28x28 크기 이기 때문에 인공 신경망에 주입하기 위해 넘파이 배열의 reshape() 메서드를 사용해 1차원으로 펼쳤는데, 케라스에서는 이를 위한 Flatten 층을 제공함\n","- Flatten 클래스는 배치 차원을 제외하고 나머지 입력 차원을 모두 일렬로 펼치는 역할만함\n","- 입력에 곱해지는 가중치나 절편 X (인공 신경망의 성능에 기여 X)\n","- 하지만 Flatten 클래스를 층처럼 입력층과 은닉층 사이에 추가하기 때문에 이를 층이라고 부름\n","- Flatten 층은 입력층 바로 뒤에 추가함"],"metadata":{"id":"teyUHJAQsgpQ"}},{"cell_type":"code","source":["model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=(28, 28)))\n","model.add(keras.layers.Dense(100, activation='relu'))\n","model.add(keras.layers.Dense(10, activation='softmax'))"],"metadata":{"id":"AoHQI7O4hyL6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 첫 번째 Dense 층에 있던 input_shape 매개변수를 Flatten 층으로 옮김\n","- 첫 번째 Dense 층의 활성화 함수를 'relu'로 변경\n","- 그러나 Flatten 층은 학습하는 층이 아니므로 이 신경망을 깊이가 3인 신경망이라고 부르지 않음"],"metadata":{"id":"6DPijYpztUzz"}},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sDrfG_BmtQCE","executionInfo":{"status":"ok","timestamp":1705231435740,"user_tz":-540,"elapsed":20,"user":{"displayName":"유찬","userId":"05761325302258896347"}},"outputId":"2bc6e168-c0e2-4060-b76e-ed8b79ec9774"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten (Flatten)           (None, 784)               0         \n","                                                                 \n"," dense_4 (Dense)             (None, 100)               78500     \n","                                                                 \n"," dense_5 (Dense)             (None, 10)                1010      \n","                                                                 \n","=================================================================\n","Total params: 79510 (310.59 KB)\n","Trainable params: 79510 (310.59 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["첫 번째 등장하는 Flatten 클래스에 포함된 모델 파라미터는 0\n","\n","**케라스의 Flatten 층을 신경망 모델에 추가하면 입력값의 차원을 짐작할 수 있는 것이 또 하나의 장점**\n","- 앞의 출력에서 784개의 입력이 첫 번째 은닉층에 전달되는 것을 확인 가능\n"," - 이전 모델에서는 쉽게 확인 불가\n"," - 입력 데이터에 대한 천처리 과정을 가능한 모델에 포함 식키는 것이 케라스 API의 철학"],"metadata":{"id":"-AsdrAtmtyDE"}},{"cell_type":"code","source":["# 훈련 데이터 준비 (이 절의 서두에 있던 코드와 동일하지만 reshape() 적용 X)\n","\n","(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()\n","\n","# 이전 코드에선 이 코드 이후 reshape 적용\n","train_scaled = train_input / 255.0\n","\n","train_scaled, val_scaled, train_target, val_target = train_test_split(\n","    train_scaled, train_target, test_size=0.2, random_state=42)"],"metadata":{"id":"bz_gYRauuW3x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')\n","\n","model.fit(train_scaled, train_target, epochs=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lp6e9Rz5tsWA","executionInfo":{"status":"ok","timestamp":1705231843984,"user_tz":-540,"elapsed":43052,"user":{"displayName":"유찬","userId":"05761325302258896347"}},"outputId":"e3e0e246-62cd-45b7-881c-abc83bc124ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1500/1500 [==============================] - 7s 4ms/step - loss: 0.5291 - accuracy: 0.8113\n","Epoch 2/5\n","1500/1500 [==============================] - 4s 3ms/step - loss: 0.3924 - accuracy: 0.8580\n","Epoch 3/5\n","1500/1500 [==============================] - 5s 3ms/step - loss: 0.3525 - accuracy: 0.8719\n","Epoch 4/5\n","1500/1500 [==============================] - 7s 4ms/step - loss: 0.3318 - accuracy: 0.8816\n","Epoch 5/5\n","1500/1500 [==============================] - 5s 4ms/step - loss: 0.3154 - accuracy: 0.8855\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7ad2dc6d1990>"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# 검증 세트에서의 성능 확인\n","\n","model.evaluate(val_scaled, val_target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Tc9hwiSvFdt","executionInfo":{"status":"ok","timestamp":1705231850931,"user_tz":-540,"elapsed":1370,"user":{"displayName":"유찬","userId":"05761325302258896347"}},"outputId":"dbba5013-56ec-474a-81d1-694f3a7c68c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["375/375 [==============================] - 1s 2ms/step - loss: 0.3865 - accuracy: 0.8668\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.3864709138870239, 0.8668333292007446]"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["시그모이드 함수를 사용했을 때, 은닉층을 추가하지 않은 경우보다 성능 향상"],"metadata":{"id":"4qusJy0SvTPL"}},{"cell_type":"markdown","source":["# 옵티마이저\n","\n","신경망에는 특히 하이퍼파라미터가 많이 존재\n","- 지금까지 다룬 하이퍼파라미터는 추가할 은닉층의 개수, 뉴런 개수, 활성화 함수, 층의 종류, 배치 사이즈 매개변수, 에포크 매개변수 등\n","\n","**추가할 은닉층의 개수, 은닉층의 뉴런 개수, 활성화 함수, 층의 종류, fit() 메서드의 배치 사이즈 매개변수, 에포크 매개 변수, 케라스의 옵티마이저, RMSprop의 학습률 모두 하이퍼파라미터**\n","- **케라스는 다양한 종류의 경사 하강법 알고리즘을 제공하는데 이들을 옵티마이저(optimizer)라고 함**\n","- 가장 기본적인 옵티마이저는 확률적 경사하강법인 SGD, 이름은 SGD이지만 1개의 샘플을 뽑아서 훈련하지 않고 앞서 언급한 것처럼 미니 배치를 사용함\n","\n"],"metadata":{"id":"Wb-9wBR45hUZ"}},{"cell_type":"code","source":["# SGD 옵티마이저를 사용하려면 compile() 메서드의 optimizer 매개변수를 'sgd'로 지정\n","model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics='accuracy')"],"metadata":{"id":"DU1xju549UCi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이 옵티마이저는 tensorflow.keras.optimizers 패키지 아래 SGD 클래스로 구현됨\n","# 'sgd' 문자열은 이 클래스의 기본 설정 매개변수로 생성한 객체와 동일, 즉 아래 코드는 위의 코드와 정확히 동일\n","\n","sgd = keras.optimizers.SGD()\n","model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy')"],"metadata":{"id":"bcj5gtGZvRWL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["원래 sgd = keras.optimizers.SGD() 처럼 SGD 클래스 객체를 만들어 사용해야 하는데 번거로움을 피하고자 'sgd'라고 지정하면 자동으로 SGD 클래스 객체를 만들어줌"],"metadata":{"id":"z2hGF6uq98Ej"}},{"cell_type":"code","source":["# SGD 클래스의 학습률 기본값이 0.01일때, 이를 바꾸고 싶으면 learning_rate 매개변수에 지정\n","\n","sgd = keras.optimizers.SGD(learning_rate=0.1)"],"metadata":{"id":"jc7TuB_H90n0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<img src= \"https://drive.google.com/uc?export=view&id=1M1Adz5_HvscsMfD7puQBvSXJUjdB1gZA\" widrh=300>\n","\n","**SGD 이외에 다양한 옵티마이저**\n","- 기본 경사 하강법 옵티마이저는 모두 SGD 클래스에서 제공\n"," - SGD 클래스의 momentum 매개변수의 기본값은 0\n"," - 이를 0보다 큰 값으로 지정하면 마치 이전의 그레이디언트를 가속도처럼 사용하는 모멘텀 최적화(momentum optimization)를 사용, 보통 momentum 매개변수는 0.9이상을 지정\n","- SGD 클래스의 nesterov 매개변수를 기본값 False에서 True로 바꾸면 네스테로프 모멘텀 최적화 (nesterov momentum optimization)(또는 네스테로프 가속 경사)를 사용"],"metadata":{"id":"DR1j9gdr-RzB"}},{"cell_type":"code","source":["sgd = keras.optimizers.SGD(momentum=0.9, nesterov=True)"],"metadata":{"id":"G8KvI8qQ-QDs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**네스테로프 모멘텀은 모멘텀 최적화를 2번 반복하여 구현**\n","- 대부분 기본 확률적 경사 하강법보다 나은 성능 제공\n","\n","모델이 최적점에 가까이 갈수록 학습률을 낮출 수 있음\n","- 이렇게 하면 안정적으로 최적점에 수렴할 가능선이 높음\n","- **이런 학습률을 적응적 학습률(adaptive learning rate)이라고 함**\n","- 이런 방식들은 학습률 매개변수를 튜닝하는 수고를 덜어줌\n","\n","**적응적 학습률을 사용하는 대표적 옵티마이저는 Adagrad와 RMSprop**\n","- 각각 compile() 메서드의 optimizer 매개변수에 'adagrad'와 'rmsprop'으로 지정\n","- optimizer 매개변수의 기본값은 'rmsprop'\n","- 이 두 옵티마이저의 매개변수를 바꾸고 싶다면 SGD와 같이 Adagrad와 RMSprop 클래스 객체를 만들어 사용하면 됨\n"],"metadata":{"id":"_ecMSk1E_FI3"}},{"cell_type":"code","source":["adagrad = keras.optimizers.Adagrad()\n","model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics='accuracy')"],"metadata":{"id":"PC-4RkPU_Dnm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rmsprop = keras.optimizers.RMSprop()\n","model.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics='accuracy')"],"metadata":{"id":"hFDigvUpAGEP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**모멘텀 최적화와 RMSprop의 장점을 접목한 것이 Adam**\n","- Adam 클래스도 keras.optimizers 패키지 아래에 있음\n","- learning_rate는 기본값으로 역시 0.001 사용"],"metadata":{"id":"7cGd3uJ8AIy0"}},{"cell_type":"code","source":["# 모델 다시 만듬\n","\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=(28, 28)))\n","model.add(keras.layers.Dense(100, activation='relu'))\n","model.add(keras.layers.Dense(10, activation='softmax'))"],"metadata":{"id":"u86eYdLzAHuc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# optimizer를 adam으로\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')\n","\n","model.fit(train_scaled, train_target, epochs=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TuhG5ey3Af9P","executionInfo":{"status":"ok","timestamp":1705236433203,"user_tz":-540,"elapsed":42817,"user":{"displayName":"유찬","userId":"05761325302258896347"}},"outputId":"76b4bdde-7c74-45e9-a36e-4a3d63fd2b5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1500/1500 [==============================] - 7s 4ms/step - loss: 0.5262 - accuracy: 0.8154\n","Epoch 2/5\n","1500/1500 [==============================] - 5s 4ms/step - loss: 0.3940 - accuracy: 0.8589\n","Epoch 3/5\n","1500/1500 [==============================] - 6s 4ms/step - loss: 0.3546 - accuracy: 0.8702\n","Epoch 4/5\n","1500/1500 [==============================] - 8s 6ms/step - loss: 0.3284 - accuracy: 0.8794\n","Epoch 5/5\n","1500/1500 [==============================] - 5s 3ms/step - loss: 0.3058 - accuracy: 0.8872\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7ad2dd7e8f70>"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["# 검증 세트 성능 확인\n","\n","model.evaluate(val_scaled, val_target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z8m6ZgNfAqIw","executionInfo":{"status":"ok","timestamp":1705236659440,"user_tz":-540,"elapsed":1399,"user":{"displayName":"유찬","userId":"05761325302258896347"}},"outputId":"442056e9-ca88-4701-e994-313cc546b4cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["375/375 [==============================] - 1s 2ms/step - loss: 0.3509 - accuracy: 0.8751\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.3508652448654175, 0.875083327293396]"]},"metadata":{},"execution_count":28}]}]}